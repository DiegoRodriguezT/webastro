[[["index.html","Pt1.html"],"Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss ‣ Álgebra lineal‣ Álgebra lineal","Skip to content. Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss Previous page Next page"],[["index.html","Pt2.html"],"Parte II Espacios vectoriales ‣ Álgebra lineal‣ Álgebra lineal","Skip to content. Espacios vectoriales Parte II Espacios vectoriales Previous page Next page"],[["index.html","Pt3.html"],"Parte III Aplicaciones lineales ‣ Álgebra lineal‣ Álgebra lineal","Skip to content. Aplicaciones lineales Parte III Aplicaciones lineales Definición 8.2. Sean \\displaystyle(E,+,\\cdot) y \\displaystyle(E^{\\prime},+,\\cdot) dos espacios vectoriales sobre el mismo cuerpo K. Diremos que la funcion \\displaystyle f:E\\to E^{\\prime} es lineal (homomorfismo) si \\displaystyle f satisface las siguientes propiedades: 1.​ \\displaystyle\\forall u,v\\in E , \\displaystyle f(u+v)=f(u)+f(v) 2.​ \\displaystyle\\forall u\\in E , \\displaystyle\\forall\\alpha\\in\\mathbb{K} , \\displaystyle f(\\alpha u)=\\alpha\\cdot f(u) Proposición 8.4. Una aplicacion \\displaystyle f:E\\to E^{\\prime} es lineal si y solo si \\displaystyle\\forall u,v\\in E , \\displaystyle\\forall\\alpha,\\beta\\in\\mathbb{K}\\quad f(\\alpha u+\\beta v)=\\alpha f% (u)+\\beta f(v) Demostración. “ \\displaystyle\\Rightarrow ” Supongamos que \\displaystyle f\\colon E\\to E^{\\prime} es lineal, o lo que es lo mismo, que \\displaystyle f satisface las condiciones \\displaystyle 1 y \\displaystyle 2 de la definicion. Dados \\displaystyle u,v\\in E y \\displaystyle\\alpha,\\beta\\in\\mathbb{K} , por la condicion 1 resulta que \\displaystyle f(\\alpha u+\\beta v)=f(\\alpha u)+f(\\beta v)=\\text{ (por la % condicion 2) }=\\alpha f(u)+\\beta f(v) “ \\displaystyle\\Leftarrow ” Reciprocamente, supongamos que \\displaystyle\\forall u,v\\in E,\\forall\\alpha,\\beta\\in\\mathbb{K} se verifica que \\displaystyle f(\\alpha u+\\beta v)=\\alpha f(u)+\\beta f(v). Hay que demostrar que \\displaystyle f satisface las condiciones 1 y 2. Dados \\displaystyle u,v\\in E , tomando \\displaystyle\\alpha=\\beta=1 , tenemos \\displaystyle f(u+v)=f(1\\cdot u+1\\cdot v)=1\\cdot f(u)+1\\cdot f(v)=f(u)+f(v). Por otro lado, si tomamos \\displaystyle\\beta=0 , resulta que \\displaystyle f(\\alpha u)=f(\\alpha u+0v)=\\alpha f(u)+0\\cdot f(v)=\\alpha f(u). ∎ Por induccion: \\displaystyle f(\\sum_{i=1}^{n}\\alpha_{i}u_{i})=\\sum_{i=1}^{n}\\alpha_{i}f(u_{i}) . Proposición 8.5. Si \\displaystyle E y \\displaystyle E^{\\prime} son \\displaystyle\\mathbb{K} -e.v. y \\displaystyle f:E\\to E^{\\prime} es una aplicacion lineal, entonces: 1.​ \\displaystyle f(0)=0 2.​ \\displaystyle\\forall u\\in E,f(-u)=-f(u) Demostración. 1.​ \\displaystyle\\forall u\\in E , \\displaystyle f(0\\cdot u)=0\\cdot f(u)=0 . 2.​Dado \\displaystyle u\\in E , se tiene \\displaystyle f(u)+f(-u)=f(u+(-u))=f(0)=0 por lo que \\displaystyle f(-u) es el opuesto de \\displaystyle f(u) , es decir, \\displaystyle f(-u)=-f(u) , puesto que \\displaystyle(E^{\\prime},+) es un grupo. ∎ Proposición 8.6. Siendo \\displaystyle E y \\displaystyle E^{\\prime} \\displaystyle\\mathbb{K} -e.v. si denotamos por \\displaystyle\\mathcal{{F}}(E,E^{\\prime}) al espacio de todas las funciones definidas entre \\displaystyle E y \\displaystyle E^{\\prime} , y por \\displaystyle\\mathcal{{L}}(E,E^{\\prime}) al conjunto de las funciones lineales definidas entre \\displaystyle E y \\displaystyle E^{\\prime} , es decir, \\displaystyle\\mathcal{{L}}(E,E^{\\prime})=\\{f\\in\\mathcal{{F}}(E,E^{\\prime})\\mid f% \\text{ es lineal}\\} se verifica que \\displaystyle\\mathcal{{L}}(E,E^{\\prime})\\leq\\mathcal{{F}}(E,E^{\\prime}) , o lo que es lo mismo, que \\displaystyle\\mathcal{{L}}(E,E^{\\prime}) tiene estructura de espacio vectorial respecto de la suma de funciones y producto de una funcion por un escalar definidos en \\displaystyle\\mathcal{{F}}(E,E^{\\prime}) . Demostración. 1.​Evidentemente \\displaystyle\\mathcal{{L}}(E,E^{\\prime})\\neq\\varnothing , puesto que \\displaystyle 0\\in\\mathcal{{L}}(E,E^{\\prime}) , ya que \\displaystyle\\forall u,v\\in E,\\forall\\alpha,\\beta\\in\\mathbb{K} se tiene \\displaystyle 0(\\alpha u+\\beta v)=0=0+0=\\alpha 0(u)+\\beta 0(v) 2.​Veamos que \\displaystyle\\forall f,g\\in\\mathcal{{L}}(E,E^{\\prime}) se verifica que \\displaystyle f+g\\in\\mathcal{{L}}(E,E^{\\prime}) . Si \\displaystyle u,v\\in E y \\displaystyle\\alpha,\\beta\\in\\mathbb{K} , se verifica que (f+g)(\\alpha u+\\beta v)=f(\\alpha u+\\beta v)+g(\\alpha u+\\beta v)=\\\\ =(\\alpha f(u)+\\beta f(v))+(\\alpha g(u)+\\beta g(v))=\\\\ =(\\alpha f(u)+\\alpha g(u))+(\\beta f(v)+\\beta g(v))=\\alpha(f+g)(u)+\\beta(f+g)(v) 3.​Veamos que \\displaystyle\\forall f\\in\\mathcal{{L}}(E,E^{\\prime}),\\forall\\lambda\\in\\mathbb{K} se verifica que \\displaystyle\\lambda f\\in\\mathcal{{L}}(E,E^{\\prime}) . Si \\displaystyle u,v\\in E y \\displaystyle\\alpha,\\beta\\in\\mathbb{K} , se verifica que (\\lambda f)(\\alpha u+\\beta v)=\\lambda\\cdot f(\\alpha u+\\beta v)=\\lambda(\\alpha f% (u)+\\beta f(v))=\\\\ =\\alpha\\lambda f(u)+\\beta\\lambda f(v)=\\alpha(\\lambda f)(u)+\\beta(\\lambda f)(v). ∎ Proposición 8.7. Si \\displaystyle E,E^{\\prime},E^{\\prime\\prime} son \\displaystyle\\mathbb{K} -e.v. y \\displaystyle f\\colon E\\to E^{\\prime} , \\displaystyle g\\colon E^{\\prime}\\to E^{\\prime\\prime} son funciones lineales, entonces la funcion composicion de \\displaystyle g con \\displaystyle f , \\displaystyle g\\circ f\\colon E\\to E^{\\prime\\prime} tambien es lineal. Demostración. Sean \\displaystyle u,v\\in E y \\displaystyle\\alpha,\\beta\\in\\mathbb{K} . En ese caso (g\\circ f)(\\alpha u+\\beta v)=g(f(\\alpha u+\\beta v))=g(\\alpha f(u)+\\beta f(v))=% \\\\ =\\alpha g(f(u))+\\beta g(f(v))=\\alpha(g\\circ f)(u)+\\beta(g\\circ f)(v). ∎ Proposición 8.8. Si \\displaystyle E y \\displaystyle E^{\\prime} son \\displaystyle\\mathbb{K} -e.v. y \\displaystyle f\\colon E\\to E^{\\prime} es lineal, se verifica que: 1.​ \\displaystyle\\forall H\\subseteq E , \\displaystyle H\\leq E\\Rightarrow f(H)\\leq E^{\\prime} 2.​ \\displaystyle\\forall H^{\\prime}\\subseteq E^{\\prime} , \\displaystyle H^{\\prime}\\leq E^{\\prime}\\Rightarrow f^{-1}(H^{\\prime})\\leq E Demostración. 1.​Supongamos que \\displaystyle H\\leq E . En ese caso \\displaystyle 0\\in H y, puesto que \\displaystyle f(0)=0 , \\displaystyle 0\\in f(H) . Por otra parte, si \\displaystyle u^{\\prime},v^{\\prime}\\in f(H) y \\displaystyle\\alpha,\\beta\\in\\mathbb{K} tendremos, por definicion de \\displaystyle f(H) , que existen \\displaystyle u,v\\in H tales que \\displaystyle f(u)=u^{\\prime} y \\displaystyle f(v)=v^{\\prime} . Luego \\displaystyle\\alpha u^{\\prime}+\\beta v^{\\prime}=\\alpha f(u)+\\beta f(v)=f(% \\alpha u+\\beta v) y, como \\displaystyle H\\leq E , \\displaystyle\\alpha u+\\beta v\\in H . Luego \\displaystyle\\alpha u^{\\prime}+\\beta v^{\\prime}\\in f(H) . Hemos llegado a que \\displaystyle f(H)\\leq E^{\\prime} . 2.​Supongamos que \\displaystyle H^{\\prime}\\leq E^{\\prime} . En ese caso \\displaystyle 0\\in H^{\\prime} y, puesto que \\displaystyle f(0)=0 , \\displaystyle 0\\in f^{-1}(H^{\\prime}) . Por otra parte, si \\displaystyle u,v\\in f^{-1}(H^{\\prime}) y \\displaystyle\\alpha,\\beta\\in\\mathbb{K} , \\displaystyle f(u),f(v)\\in H^{\\prime} . Como \\displaystyle H^{\\prime}\\leq E^{\\prime} , \\displaystyle\\alpha f(u)+\\beta f(v)\\in H^{\\prime}\\Rightarrow f(\\alpha u+\\beta v% )\\in H^{\\prime} y \\displaystyle\\alpha u+\\beta v\\in f^{-1}(H^{\\prime}) . ∎ Definición 8.3. Sean \\displaystyle E y \\displaystyle E^{\\prime} \\displaystyle\\mathbb{K} -e.v. y \\displaystyle f\\colon E\\to E^{\\prime} una funcion lineal, llamaremos nucleo de \\displaystyle f al conjunto \\displaystyle Ker(f)=f^{-1}({0})=\\{u\\in E\\mid f(u)=0\\} e imagen de \\displaystyle f al conjunto \\displaystyle Im(f)=\\{v\\in E^{\\prime}\\mid\\exists u\\in E\\mid f(u)=v\\}=f(E) Observación. \\displaystyle Ker(f)\\leq E y \\displaystyle Im(f)\\leq E^{\\prime} . Definición 8.4. Sean \\displaystyle E,E^{\\prime} \\displaystyle\\mathbb{K} -e.v. y \\displaystyle f\\colon E\\to E^{\\prime} una funcion lineal, a la dimension de la imagen de \\displaystyle f se le denomina rango de \\displaystyle f y a la dimension del nucleo nulidad. Ejemplo. 1.​El nucleo de la funcion identidad \\displaystyle Id_{E}\\colon E\\to E es el subespacio \\displaystyle\\{0\\} y su imagen es el subespacio \\displaystyle E . 2.​Sea \\displaystyle f:\\mathbb{R}^{3}\\to\\mathbb{R}^{3} la proyeccion ortogonal sobre el plano \\displaystyle z=0 . \\displaystyle Ker(f)=\\{(x,y,z)\\mid f(x,y,0)=(0,0,0)\\}=\\{(x,y,z)\\mid x=y=0\\} Teorema 8.1. Sean \\displaystyle E y \\displaystyle E^{\\prime} \\displaystyle\\mathbb{K} -e.v. y \\displaystyle f\\colon E\\to E^{\\prime} una funcion lineal. Se verifica que: 1.​ \\displaystyle f es inyectiva \\displaystyle\\Leftrightarrow \\displaystyle Ker(f)=\\{0\\} . 2.​ \\displaystyle f es suprayectiva \\displaystyle\\Leftrightarrow Im(f)=E^{\\prime} . Demostración. 1.​“ \\displaystyle\\Rightarrow ” Puesto que \\displaystyle f es lineal, \\displaystyle f(0)=0 y en consecuencia \\displaystyle 0\\in Ker(f) . Si \\displaystyle u\\in E es tal que \\displaystyle u\\in Ker(f) , entonces \\displaystyle f(u)=0 y \\displaystyle f(0)=0 . Como partimos de la hipoteiss de que \\displaystyle f es inyectiva, llegamos a que \\displaystyle u=0 . Por tanto, \\displaystyle Ker(f)=\\{0\\} . “ \\displaystyle\\Leftarrow ” Supongamos que \\displaystyle Ker(f)=\\{0\\} y que \\displaystyle u,v\\in E son tales que \\displaystyle f(u)=f(v) . Entonces \\displaystyle f(u)+(-f(v))=0\\Rightarrow f(u)+f(-v)=0\\Rightarrow f(u+(-v))=0% \\Rightarrow u+(-v)\\in Ker(f)\\Rightarrow u+(-v)=0 . Por tanto, \\displaystyle u=v y llegamos a que \\displaystyle f es inyectiva. 2.​Por definicion, \\displaystyle f es sobreyectiva si y solo si \\displaystyle Im(f)=E^{\\prime} . ∎ Definición 8.5. Siendo \\displaystyle E y \\displaystyle E^{\\prime} dos \\displaystyle\\mathbb{K} -e.v., se dice que \\displaystyle E y \\displaystyle E^{\\prime} son isomorfos si existe una funcion \\displaystyle f\\colon E\\to E^{\\prime} tal que \\displaystyle f es lineal y biyectiva. En ese caso, se dice que \\displaystyle f es un isomorfismo. Proposición 8.9. Si \\displaystyle f\\colon E\\to E^{\\prime} es un isomorfismo, \\displaystyle f^{-1}\\colon E^{\\prime}\\to E es tambien un isomorfismo. Demostración. Si \\displaystyle f\\colon E\\to E^{\\prime} es biyectiva entonces \\displaystyle f^{-1}\\colon E^{\\prime}\\to E es tambien biyectiva. Veamos que es lineal. 1.​Sean \\displaystyle u^{\\prime},v^{\\prime}\\in E^{\\prime} . Por ser \\displaystyle f\\colon E\\to E^{\\prime} biyectiva, \\displaystyle\\exists!u,v\\in E tales que \\displaystyle f(u)=u^{\\prime} y \\displaystyle f(v)=v^{\\prime} . Luego \\displaystyle f^{-1}(u^{\\prime}+v^{\\prime})=f^{-1}(f(u)+f(v))=f^{-1}(f(u+v))=u% +v=f^{-1}(u)+f^{-1}(v) . 2.​Sean \\displaystyle u^{\\prime}\\in E^{\\prime} y \\displaystyle\\alpha\\in\\mathbb{K} . Entonces \\displaystyle\\exists!u\\in E\\mid f(u)=u^{\\prime} . Tenemos que \\displaystyle f^{-1}(\\alpha u^{\\prime})=f^{-1}(\\alpha f(u))=f^{-1}(f(\\alpha u)% )=\\alpha u=\\alpha f^{-1}(u^{\\prime}). ∎ Teorema 8.2. Sean \\displaystyle E y \\displaystyle E^{\\prime} dos \\displaystyle\\mathbb{K} -e.v., \\displaystyle\\{u_{1},\\ldots,u_{n}\\} , base de E, \\displaystyle\\{v_{1},\\ldots,v_{n}\\} un sistema de cualquiera de \\displaystyle n vectores de \\displaystyle E^{\\prime} . En estas condiciones existe una unica funcion lineal \\displaystyle f\\colon E\\to E^{\\prime} tal que \\displaystyle\\forall i\\in\\{1,\\ldots,n\\},f(u_{i})=v_{i} . Ademas se verifica: 1.​ \\displaystyle f es inyectiva \\displaystyle\\Leftrightarrow\\{v_{1},\\ldots,v_{n}\\} es libre. 2.​ \\displaystyle f es sobrectiva \\displaystyle\\Leftrightarrow\\langle v_{1},\\ldots,v_{n}\\rangle=E^{\\prime} 3.​ \\displaystyle f es biyectiva \\displaystyle\\Leftrightarrow\\{v_{1},\\ldots,v_{n}\\} es base de \\displaystyle E^{\\prime} . Demostración. i)​Existencia. Sea \\displaystyle u\\in E , como \\displaystyle B=\\{u_{1},\\ldots,u_{n}\\} \\displaystyle\\exists!(\\alpha_{1},\\ldots,\\alpha_{n})\\in\\mathbb{K}^{n} tal que \\displaystyle u=\\sum_{i=1}^{n}\\alpha_{i}u_{i} . Entonces definimos \\displaystyle f(u)=\\sum_{i=1}^{n}\\alpha_{i}v_{i} . Veamos que la funcion \\displaystyle f asi definida para cada \\displaystyle u\\in E satisface las dos condiciones requeridas. a)​ \\displaystyle f es lineal: dados \\displaystyle u,v\\in E y \\displaystyle\\lambda,\\mu\\in\\mathbb{K} , si \\displaystyle u=\\sum_{i=1}^{n}\\alpha_{i}u_{i} y \\displaystyle v=\\sum_{i=1}^{n}\\beta_{i}u_{i} , resulta que \\displaystyle f(\\lambda u+\\mu v)=f(\\lambda\\cdot(\\sum_{i=1}^{n}\\alpha_{i}u_{i})% +\\mu\\cdot(\\sum_{i=1}^{n}\\beta_{i}u_{i}))=f((\\sum_{i=1}^{n}(\\lambda\\alpha_{i}+% \\mu\\beta_{i})u_{i}))=\\sum_{i=1}^{n}(\\lambda\\alpha_{i}+\\mu\\beta_{i})v_{i}=% \\lambda(\\sum_{i=1}^{n}\\alpha v_{i})+\\mu(\\sum_{i=1}^{n}\\beta_{i}u_{i})=\\lambda f% (u)+\\mu f(v) . b)​ \\displaystyle\\forall i\\in\\{1,\\ldots,n\\} , \\displaystyle f(u_{i})=v_{i} . Dado \\displaystyle i\\in\\{1,\\ldots,n\\} , se tiene \\displaystyle u_{i}=0\\cdot u_{1}+\\cdots+1\\cdot u_{i}+\\cdots 0\\cdot u_{n} y por tanto \\displaystyle f(u_{i})=0\\cdot v_{1}+\\cdots+1\\cdot v_{i}+\\cdots+0\\cdot v_{n} ii)​Unicidad. Si \\displaystyle g\\colon E\\to E^{\\prime} es una funcion lineal tal que \\displaystyle\\forall i\\in\\{1,\\ldots,n\\}\\;g(u_{i})=v_{i} , siendo \\displaystyle w un vector cualquiera de \\displaystyle E , \\displaystyle\\exists(\\alpha_{1},\\ldots,\\alpha_{n})\\in\\mathbb{K}^{n} tal que \\displaystyle w=\\sum_{i=1}^{n}\\alpha_{i}u_{i}. Entonces \\displaystyle g(w)=g(\\sum_{i=1}^{n}\\alpha_{i}u_{i})=\\sum_{i=1}^{n}\\alpha_{i}g(% u_{i})=\\sum_{i=1}^{n}\\alpha_{i}v_{i}=\\sum_{i=1}^{n}\\alpha_{i}f(u_{i})=f(\\sum_{% i=1}^{n}\\alpha_{i}u_{i})=f(w) . Luego \\displaystyle f=g . Por ultimo, comprobamos que se verifican a), b) y c). a)​“ \\displaystyle\\Rightarrow ” Sea \\displaystyle\\sum_{i=1}^{n}\\alpha_{i}v_{i}=0 . Entonces \\displaystyle 0=\\sum_{i=1}^{n}\\alpha_{i}v_{i}=\\sum_{i=1}^{n}\\alpha_{i}f(u_{i})% =f(\\sum_{i=1}^{n}\\alpha_{i}u_{i}). Ya que \\displaystyle f es inyectiva, \\displaystyle\\sum_{i=1}^{n}\\alpha_{i}u_{i}=0 y, como \\displaystyle\\{u_{1},\\ldots,u_{n}\\} es libre, \\displaystyle\\alpha_{1}=\\cdots=\\alpha_{n}=0 . “ \\displaystyle\\Leftarrow ” Si \\displaystyle w\\in Ker(f) , siendo \\displaystyle w=\\sum_{i=1}^{n}\\alpha_{i}u_{i} , tenemos que \\displaystyle f(w)=0=f(\\sum_{i=1}^{n}\\alpha_{i}u_{i})=\\sum_{i=1}^{n}\\alpha_{i}% f(u_{i})=\\sum_{i=1}^{n}\\alpha_{i}v_{i} y como \\displaystyle\\{v_{1},\\ldots,v_{n}\\} es libre, \\displaystyle\\alpha_{1}=\\cdots=\\alpha_{n}=0 . Por tanto, \\displaystyle w=0 y \\displaystyle f es inyectiva. b)​Tenemos que probar que \\displaystyle Im(f)=\\langle v_{1},\\ldots,v_{n}\\rangle . En ese caso, \\displaystyle f es sobreyectiva si y solo si \\displaystyle\\langle v_{1},\\ldots,v_{n}\\rangle=E^{\\prime} . Como \\displaystyle\\langle v_{1},\\ldots,v_{n}\\rangle\\leq Im(f) , solo hace falta comprobar que \\displaystyle Im(f)\\subseteq\\langle v_{1},\\ldots,v_{n}\\rangle . Si \\displaystyle v\\in Im(f) , existe \\displaystyle u=\\sum_{i=1}^{n}\\alpha_{i}u_{i}\\in E tal que \\displaystyle v=f(u)=f(\\sum_{i=1}^{n}\\alpha_{i}u_{i})=\\sum_{i=1}^{n}\\alpha_{i}% f(u_{i})=\\sum_{i=1}^{n}\\alpha_{i}v_{i}\\in\\langle v_{1},\\ldots,v_{n}\\rangle. Asi que \\displaystyle Im(f)=\\langle v_{1},\\ldots,v_{n}\\rangle=E^{\\prime} . c)​Es consecuencia de los dos apartados anteriores. ∎ Corolario 8.1. Si \\displaystyle E y \\displaystyle E^{\\prime} son \\displaystyle\\mathbb{K} -e.v. y \\displaystyle\\{u_{1},\\ldots,u_{n}\\} es una base de \\displaystyle E , cualquier funcion lineal \\displaystyle f\\colon E\\to E^{\\prime} queda completamente determinada por el sistema \\displaystyle\\{f(u_{1}),\\ldots,f(u_{n})\\} . Corolario 8.2. Si \\displaystyle E y \\displaystyle E^{\\prime} son dos \\displaystyle\\mathbb{K} -e.v. de dimension finita, entonces \\displaystyle E\\text{ y }E^{\\prime}\\text{ son isomorfos}\\Leftrightarrow dim(E)% =dim(E^{\\prime}) Demostración. “ \\displaystyle\\Rightarrow ” Si \\displaystyle E y \\displaystyle E^{\\prime} son isomorfos, \\displaystyle f\\colon E\\to E^{\\prime} es un isomorfismo y \\displaystyle\\{u_{1},\\ldots,u_{n}\\} es una base de \\displaystyle E . Por el teorema anterior, \\displaystyle\\{f(u_{1}),\\ldots,f(u_{n})\\} es una base de \\displaystyle E^{\\prime} y, por tanto, \\displaystyle dim(E)=dim(E^{\\prime})=n . “ \\displaystyle\\Leftarrow ” Si \\displaystyle dim(E)=dim(E^{\\prime})=n , \\displaystyle\\{u_{1},\\ldots,u_{n}\\} es una base de \\displaystyle E y \\displaystyle\\{v_{1},\\ldots,v_{n}\\} es una base de \\displaystyle E^{\\prime} . Si consideramos la funcion lineal \\displaystyle f\\colon E\\to E^{\\prime} tal que \\displaystyle\\forall i\\in\\{1,\\ldots,n\\}\\;f(u_{i})=v_{i} , tenemos que \\displaystyle f es un isomorfismo (demostrado anteriormente) y \\displaystyle E y \\displaystyle E^{\\prime} son isomorfos. ∎ Teorema 8.3 (de la dimension para funciones lineales). Si \\displaystyle f\\colon E\\to E^{\\prime} es una funcion lineal tal que los subespacios \\displaystyle Ker(f) e \\displaystyle Im(f) son de dimension finita, entonces \\displaystyle E es tambien de dimension finita y su dimension viene dada por \\displaystyle dim(E)=dim(Ker(f))+dim(Im(f)). Demostración. Sean \\displaystyle\\{u_{1},\\ldots,u_{r}\\} una base de \\displaystyle Ker(f) y \\displaystyle\\{v_{1},\\ldots,v_{p}\\} una base de \\displaystyle Im(f) . Sean, por otra parte, \\displaystyle\\{w_{1},\\ldots,w_{p}\\} tales que \\displaystyle\\forall i\\in\\{1,\\ldots,p\\},\\;f(w_{i})=v_{i} . El teorema quedara demostrado si comprobamos que \\displaystyle\\{u_{1},\\ldots,u_{r},w_{1},\\ldots,w_{p}\\} es una base de \\displaystyle E . a)​ \\displaystyle\\{u_{1},\\ldots,u_{r},w_{1},\\ldots,w_{p}\\} es libre. Existen \\displaystyle\\alpha_{1},\\ldots,\\alpha_{r},\\beta_{1},\\ldots,\\beta_{p} tal que \\displaystyle\\sum_{i=1}^{r}\\alpha_{i}u_{i}+\\sum_{j=1}^{p}\\beta_{j}w_{j}=0. Entonces \\displaystyle f(\\sum_{i=1}^{r}\\alpha_{i}u_{i}+\\sum_{j=1}^{p}\\beta_{j}w_{j})=f(% 0)=0 . Como \\displaystyle f es lineal, \\displaystyle\\sum_{i=1}^{r}\\alpha_{i}f(u_{i})+\\sum_{j=1}^{p}\\beta_{j}f(w_{j})=0 y puesto que los vectores \\displaystyle u_{1},\\ldots,u_{r}\\in Ker(f) , \\displaystyle\\sum_{j=1}^{p}\\beta_{j}v_{j}=0 . Al ser \\displaystyle\\{v_{1},\\ldots,v_{p}\\} libre, obtenemos que \\displaystyle\\beta_{1}=\\cdots=\\beta_{p}=0 . Volviendo a la ecuacion inicial, nos queda \\displaystyle\\sum_{i=1}^{r}\\alpha_{i}u_{i}=0 , y como \\displaystyle\\{u_{1},\\ldots,u_{r}\\} es libre, \\displaystyle\\alpha_{1}=\\cdots=\\alpha_{r}=0 . Por lo tanto, \\displaystyle\\alpha_{1}=\\cdots=\\alpha_{r}=\\beta_{1}=\\cdots=\\beta_{p}=0 y es un sistema libre. 1.​ \\displaystyle\\{u_{1},\\ldots,u_{r},w_{1},\\ldots,w_{p}\\} es un sistema generador de \\displaystyle E . Sea \\displaystyle u\\in E . Como \\displaystyle f(u)\\in Im(f) y \\displaystyle\\{v_{1},\\ldots,v_{p}\\} es una base de \\displaystyle Im(f) , \\displaystyle\\exists\\beta_{1},\\ldots,\\beta_{p} tal que \\displaystyle f(u)=\\sum_{i=1}^{p}\\beta_{i}v_{i} . Por tanto, \\displaystyle f(u)=\\sum_{i=1}^{p}\\beta_{i}v_{i}=\\sum_{i=1}^{p}\\beta_{i}f(w_{i}% )=f(\\sum_{i=1}^{p}\\beta_{i}w_{i}). Luego \\displaystyle 0=f(u)-f(\\sum_{i=1}^{p}\\beta_{i}w_{i})=f(u-\\sum_{i=1}^{p}\\beta_{% i}w_{i})\\in Ker(f) Como \\displaystyle Ker(f)=\\langle u_{1},\\ldots,u_{r}\\rangle , \\displaystyle\\exists\\alpha_{1},\\ldots,\\alpha_{r} tal que \\displaystyle u-\\sum_{i=1}^{p}\\beta_{i}w_{i}=\\sum_{j=1}^{r}\\alpha_{j}u_{j}\\Rightarrow \\displaystyle u=\\sum_{i=1}^{p}\\beta_{i}w_{i}+\\sum_{j=1}^{r}\\alpha_{j}u_{j} ∎ Previous page"],[["index.html","Pt1.html","S1.html"],"1 Nociones basicas ‣ Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss ‣ Álgebra lineal‣ Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss ‣ Álgebra lineal","Skip to content. Nociones basicas 1 Nociones basicas Definición 1.1. Un conjunto (denotado con letras mayusculas \\displaystyle A,B,C, etc.) es una coleccion de elementos, usualmente denotados con letras minusculas (a,b,c,etc.). En los conjuntos no importa el orden de los elementos. Si queremos indicar todos los elementos que pertenecen a un conjunto, los indicaremos entre llaves. Por ejemplo, \\displaystyle A=\\{a,b,c\\} Para indicar que un elemento \\displaystyle x pertenece a un conjunto \\displaystyle A escribiremos \\displaystyle x\\in A . El conjunto que no contiene ningun elemento se llama conjunto vacio y se escribe \\displaystyle\\varnothing . Ejemplo. \\displaystyle\\mathbb{N} es el conjunto de los numeros naturales. \\displaystyle\\mathbb{Z} es el conjunto de los numeros enteros. \\displaystyle\\mathbb{Q} es el conjunto de los numeros racionales. \\displaystyle\\mathbb{R} es el conjunto de los numeros reales. \\displaystyle\\mathbb{C} es el conjunto de los numeros complejos. \\displaystyle\\mathbb{C}=\\{a+bi\\colon a,b\\in\\mathbb{R}\\} . Definición 1.2. El producto cartesiano de dos conjuntos \\displaystyle A y \\displaystyle B se denota como \\displaystyle A\\times B y se define por \\displaystyle A\\times B=\\{(a,b)\\mid a\\in A,b\\in B\\} Es decir, es el conjunto de todas las posibles parejas donde el primer numero pertenece al conjunto \\displaystyle A y el segundo pertenece al conjunto \\displaystyle B . El producto cartesiano \\displaystyle A\\times A se escribe \\displaystyle A^{2} . En caso general, se define \\displaystyle A^{n} como \\displaystyle A^{n}=\\{(x_{1},x_{2},\\ldots,x_{n})\\mid x_{1},x_{2},\\dots,x_{n}% \\in A\\} Definición 1.3. Una operacion binaria interna \\displaystyle* de un conjunto \\displaystyle X es una aplicacion \\displaystyle*\\colon X\\times X \\displaystyle\\longrightarrow X \\displaystyle(x,y) \\displaystyle\\longmapsto x*y que a cada elemento \\displaystyle(x,y)\\in X\\times X le asigna un unico elemento de \\displaystyle X , denotado por \\displaystyle x*y . Para indicar que \\displaystyle X tiene una operacion binaria interna \\displaystyle* habitualmente escribiremos \\displaystyle(X,*) . Ejemplo. En el conjunto de los numeros naturales, se pueden definir dos operaciones binarias internas: \\displaystyle+\\colon\\mathbb{N}\\times\\mathbb{N}\\longrightarrow\\mathbb{N},(a,b)% \\longmapsto a+b \\displaystyle\\cdot\\colon\\mathbb{N}\\times\\mathbb{N}\\longrightarrow\\mathbb{N} Decimos que \\displaystyle e\\in X es elemento neutro de \\displaystyle(X,*) si \\displaystyle e*x=x*e=x para todo \\displaystyle x\\in X . La operación interna en \\displaystyle X es asociativa si \\displaystyle x*(y*z)=(x*y)*z\\;\\forall x,y,z\\in X . La operación interna \\displaystyle* en \\displaystyle X es conmutativa si \\displaystyle x*y=y*x\\;\\forall x,y\\in X . El inverso de un elemento \\displaystyle x\\in X respecto de la operación interna \\displaystyle* , si existe, es otro elemento \\displaystyle y\\in X tal que \\displaystyle x*y=y*x=e . El inverso de cada elemento es único cuando \\displaystyle* es asociativa. Demostración. Supongamos que existe \\displaystyle y_{1},y_{2} tal que \\displaystyle y_{1}*x=e y \\displaystyle y_{2}*x=e , con \\displaystyle* asociativa. \\displaystyle y_{1}=e*y_{1}=(y_{2}*x)*y_{1}=y_{2}*(x*y_{1})=y_{2}*e=y_{2} ∎ El inverso de \\displaystyle x respecto de \\displaystyle* se suele denotar \\displaystyle x^{-1} . Caso particular: si la operacion interna en \\displaystyle X es la suma \\displaystyle+ , el elemento neutro se denota como \\displaystyle 0 y el inverso, si existe, de un elemento \\displaystyle x se suele llamar opuesto y se denota como \\displaystyle-x . Definición 1.4 (Grupo). Un conjunto \\displaystyle G con una operacion binaria interna \\displaystyle* es un grupo si tiene elemento neutro, si \\displaystyle* es asociativa y si todo elemento de \\displaystyle G tiene inverso. Si ademas \\displaystyle* es conmutativa, \\displaystyle(G,*) es un grupo conmutativo o abeliano. Definición 1.5 (Anillo). Un conjunto \\displaystyle A con dos operaciones internas, denotadas \\displaystyle+ y \\displaystyle\\cdot , es un anillo si cumple:  ​ \\displaystyle(A,+) es un grupo abeliano.  ​ \\displaystyle\\cdot es asociativa.  ​Distributividad: \\displaystyle x\\cdot(y+z)=x\\cdot y+x\\cdot z\\quad\\forall x,y,z\\in A Si, ademas, existe elemento neutro respecto de la operación \\displaystyle\\cdot , será un anillo unitario. Si \\displaystyle\\cdot es conmutativa, \\displaystyle A es un anillo conmutativo. Definición 1.6 (Cuerpo). Un cuerpo es un anillo conmutativo unitario \\displaystyle(X,+,\\cdot) en el que, además, todo elemento distinto del elemento neutro de la suma tiene inverso respecto de \\displaystyle\\cdot . Previous page Next page"],[["index.html","Pt1.html","S2.html"],"2 Matrices ‣ Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss ‣ Álgebra lineal‣ Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss ‣ Álgebra lineal","Skip to content. Matrices 2 Matrices Definición 2.1. Sea \\displaystyle\\mathbb{K} un cuerpo y sean \\displaystyle m,n\\in\\mathbb{N} . Una matriz \\displaystyle m\\times n sobre \\displaystyle\\mathbb{K} es una tabla rectangular formada por \\displaystyle m filas y \\displaystyle n columnas de elementos de \\displaystyle\\mathbb{K} : \\displaystyle A=\\begin{pmatrix}a_{11}&a_{12}&\\cdots&a_{1n}\\\\ a_{21}&a_{22}&\\cdots&a_{2n}\\\\ \\vdots&\\vdots&\\ddots&\\vdots\\\\ a_{m1}&a_{m2}&\\cdots&a_{mn}\\\\ \\end{pmatrix} donde \\displaystyle a_{ij}\\in\\mathbb{K} , \\displaystyle i=1,\\ldots,m , \\displaystyle j=1,\\ldots,n .  ​ \\displaystyle a_{ij} es el elemento \\displaystyle(ij) de la matriz \\displaystyle A , y se llama coeficiente de la matriz,  ​ \\displaystyle i es el índice de fila,  ​ \\displaystyle j es el índice de columna,  ​los elementos \\displaystyle a_{11},a_{22},\\ldots,a_{pp} (donde \\displaystyle p=min\\{m,n\\} ) se llaman elementos diagonales y \\displaystyle\\begin{pmatrix}a_{11}&a_{22}&\\cdots&a_{pp}\\\\ \\end{pmatrix} se llama diagonal principal de \\displaystyle A .  ​Notacion: \\displaystyle A=(a_{ij})_{i,j} Si \\displaystyle m=n , \\displaystyle A se llama matriz cuadrada de orden \\displaystyle n . Si \\displaystyle m=1 , \\displaystyle A se llama matriz fila. Si \\displaystyle n=1 , \\displaystyle A se llama matriz columna. Si \\displaystyle i_{1},i_{2},\\ldots,i_{p} son algunos de los indices de fila de \\displaystyle A , y \\displaystyle j_{1},j_{2},\\ldots,j_{q} son algunos de los índices de columna de \\displaystyle A , la matriz \\displaystyle p\\times q formada por las filas y columnas correspondientes a los índices señalados se llama submatriz de \\displaystyle A . Cabe destacar que en una matriz \\displaystyle A de tamaño \\displaystyle m\\times n cada fila de \\displaystyle A viene dada por \\displaystyle n elementos de \\displaystyle K y puede interpretarse como un elemento de \\displaystyle K^{n}:A_{1},\\ldots,A_{m}\\in K^{n} . Del mismo modo, cada columna de \\displaystyle A viene dada por \\displaystyle m elementos de \\displaystyle K y puede interpretarse como un elemento de \\displaystyle K^{m}:A^{1},\\ldots,A^{n}\\in K^{m} . Definición 2.2. Dado un cuerpo \\displaystyle\\mathbb{K} , denotamos \\displaystyle\\mathrm{Mat}_{m\\times n}(\\mathbb{K})=\\mathfrak{M}_{m\\times n}(% \\mathbb{K})=\\{A\\mid A\\text{ matriz }m\\times n\\text{ sobre }\\mathbb{K}\\} \\displaystyle\\mathrm{Mat}_{n}(\\mathbb{K})=\\mathfrak{M}_{n}(\\mathbb{K})=\\mathrm% {Mat}_{n\\times n}(\\mathbb{K}) Definición 2.3 (Tipos de matrices). 1.​Una matriz \\displaystyle A\\in\\mathfrak{M}_{n}(\\mathbb{K}) se llama matriz diagonal si \\displaystyle a_{ij}=0 para todo \\displaystyle i\\neq j : \\displaystyle A=\\begin{pmatrix}a_{11}&&0\\\\ &\\ddots&\\\\ 0&&a_{nn}\\\\ \\end{pmatrix} 2.​Una matriz diagonal \\displaystyle A tal que \\displaystyle a_{11}=a_{22}=\\cdots=a_{nn} se llama matriz escalar: \\displaystyle A=\\begin{pmatrix}\\lambda&&0\\\\ &\\ddots&\\\\ 0&&\\lambda\\\\ \\end{pmatrix} Si \\displaystyle\\lambda=1 , la matriz \\displaystyle A se llama matriz identidad y se denota \\displaystyle I . 3.​La matriz \\displaystyle m\\times n tal que todos sus elementos son \\displaystyle 0_{K} se llama matriz nula: \\displaystyle 0_{mn}=\\begin{pmatrix}0_{K}&\\cdots&0_{K}\\\\ \\vdots&\\ddots&\\vdots\\\\ 0_{K}&\\cdots&0_{K}\\\\ \\end{pmatrix} 4.​Una matriz cuadrada \\displaystyle A\\in\\mathfrak{M}_{n}(\\mathbb{K}) se llama matriz triangular superior si \\displaystyle a_{ij}=0 para todo \\displaystyle i>j : \\displaystyle A=\\begin{pmatrix}a_{11}&a_{12}&\\cdots&a_{1n}\\\\ 0_{K}&a_{22}&\\cdots&a_{2n}\\\\ \\vdots&\\ddots&\\ddots&\\vdots\\\\ 0_{K}&\\cdots&0_{K}&a_{nn}\\\\ \\end{pmatrix} 5.​Una matriz cuadrada \\displaystyle A\\in\\mathfrak{M}_{n}(\\mathbb{K}) se llama matriz triangular inferior si \\displaystyle a_{ij}=0 para todo \\displaystyle i<j . \\displaystyle A=\\begin{pmatrix}a_{11}&0_{K}&\\cdots&0_{K}\\\\ a_{12}&a_{22}&\\ddots&\\vdots\\\\ \\vdots&\\vdots&\\ddots&0_{K}\\\\ a_{n1}&a_{n2}&\\cdots&a_{nn}\\\\ \\end{pmatrix} 6.​ \\displaystyle E_{ij}\\in\\mathfrak{{M}}_{m\\times n}(\\mathbb{K}) denota la matriz que tiene todos sus elementos nulos, salvo el elemento \\displaystyle(ij) (fila \\displaystyle i columna \\displaystyle j ) que es \\displaystyle 1_{K} . Definición 2.4. Dada \\displaystyle A\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K}) , 1.​la matriz opuesta de \\displaystyle A es \\displaystyle-A=(-a_{ij})_{ij}\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K}) 2.​la matriz traspuesta de \\displaystyle A es \\displaystyle A^{t}=(a_{ji})_{ji}\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K}) Ejemplo. Si \\displaystyle A=\\begin{pmatrix}1&2&3\\\\ 4&5&6\\\\ \\end{pmatrix} , entonces \\displaystyle-A=\\begin{pmatrix}-1&-2&-3\\\\ -4&-5&-6\\\\ \\end{pmatrix}\\text{ y }A^{t}=\\begin{pmatrix}1&4\\\\ 2&5\\\\ 3&6\\\\ \\end{pmatrix} Definición 2.5. Dada una matriz cuadrada \\displaystyle A\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K}) ,  ​se dice que \\displaystyle A es simetrica si \\displaystyle A^{t}=A  ​se dice que \\displaystyle A es una matriz antisimetrica si \\displaystyle A^{t}=-A Definición 2.6 (Suma de matrices). En \\displaystyle\\mathfrak{M}_{m\\times n}(\\mathbb{K}) se define la suma de matrices del siguiente modo: \\displaystyle A+B=(a_{ij}+b_{ij})_{ij}\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K}) para toda \\displaystyle A=(a_{ij})_{ij} , \\displaystyle B=(b_{ij})_{ij} en \\displaystyle\\mathfrak{M}_{m\\times n}(\\mathbb{K}) . Es decir, para sumar dos matrices, ambas han de tener el mismo tamaño (mismo número de filas y mismo número de columnas) y la suma se realiza componente a componente (en cada una de las posiciones \\displaystyle(ij) de la matriz \\displaystyle A+B escribiremos \\displaystyle a_{ij}+b_{ij} ). Proposición 2.1. \\displaystyle(\\mathfrak{{M}}_{m\\times n},+) es un grupo conmutativo. Demostración. Claramente, la suma de matrices satisface la propiedad asociativa y la propiedad conmutativa (porque son propiedades que se cumplen en \\displaystyle\\mathbb{K} ). Además, \\displaystyle\\mathfrak{{M}}_{m\\times n}(\\mathbb{K}) tiene por elemento neutro de la suma a la matriz nula \\displaystyle 0_{m\\times n} , y todo elemento \\displaystyle A de \\displaystyle\\mathfrak{{M}}_{m\\times n}(\\mathbb{K}) tiene por opuesto a su matriz opuesta \\displaystyle-A . ∎ Definición 2.7 (Producto de una matriz por un escalar). Dada una matriz \\displaystyle A\\in\\mathfrak{{M}}_{m\\times n}(\\mathbb{K}) y un escalar \\displaystyle t\\in\\mathbb{K} , definimos el producto de \\displaystyle t por \\displaystyle A=(a_{ij})_{ij} como la matriz de \\displaystyle\\mathfrak{{M}}_{m\\times n}(\\mathbb{K}) cuyo elemento en la posicion \\displaystyle(ij) es \\displaystyle ta_{ij} . Lo denotamos por \\displaystyle tA . Definición 2.8 (Producto de matrices). Dadas \\displaystyle A=(a_{ij})_{ij}\\in\\mathfrak{{M}}_{m\\times n}(\\mathbb{K}) y \\displaystyle B=(b_{ij})_{ij}\\in\\mathfrak{{M}}_{n\\times p}(\\mathbb{K}) , definimos el producto de \\displaystyle A por \\displaystyle B como la matriz \\displaystyle C=(c_{ij})_{ij}\\in\\mathfrak{{M}}_{m\\times p}(\\mathbb{K}) cuyos elementos \\displaystyle c_{ij} son \\displaystyle c_{ij}=\\begin{pmatrix}a_{i1}&\\cdots&a_{in}\\\\ \\end{pmatrix}\\begin{pmatrix}b_{1j}\\\\ \\vdots\\\\ b_{nj}\\\\ \\end{pmatrix}=\\sum_{k=1}^{n}a_{ik}b_{kj} Notemos que solo se pueden multiplicar dos matrices \\displaystyle A y \\displaystyle B cuando el número de columnas de \\displaystyle A coincide con el número de filas de \\displaystyle B . Proposición 2.2 (Propiedades del producto de matrices). Las propiedades del producto de matrices son las siguientes: 1.​El producto no es conmutativo en general. 2.​ \\displaystyle I_{m}A=A=AI_{n} para todo \\displaystyle A\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K}) . 3.​En general, \\displaystyle AB=0\\not\\Rightarrow A=0\\text{ o }B=0 4.​El producto es asociativo: \\displaystyle(AB)C=A(BC) 5.​ \\displaystyle A(B_{1}+B_{2})=AB_{1}+AB_{2} 6.​ \\displaystyle(A_{1}+A_{2})B=A_{1}B+AB_{2} 7.​ \\displaystyle(tA)B=t(AB)=A(tB) Demostración. 1.​Por ejemplo, \\displaystyle AB=\\begin{pmatrix}1&1\\\\ 0&1\\\\ \\end{pmatrix}\\cdot\\begin{pmatrix}1&1\\\\ 1&0\\\\ \\end{pmatrix}=\\begin{pmatrix}2&1\\\\ 1&0\\\\ \\end{pmatrix} \\displaystyle BA=\\begin{pmatrix}1&1\\\\ 1&0\\\\ \\end{pmatrix}\\cdot\\begin{pmatrix}1&1\\\\ 0&1\\\\ \\end{pmatrix}=\\begin{pmatrix}1&2\\\\ 1&1\\\\ \\end{pmatrix} 2.​Es trivial. 3.​Tomar por ejemplo \\displaystyle A=\\begin{pmatrix}0&1\\\\ 0&0\\\\ \\end{pmatrix} y \\displaystyle B=\\begin{pmatrix}1&0\\\\ 0&0\\\\ \\end{pmatrix} , que cumplen \\displaystyle AB=0 . Las propiedades 4,5,6 y 7 se deducen de las correspondientes propiedades en el cuerpo \\displaystyle\\mathbb{K} . ∎ Proposición 2.3. 1.​Dados \\displaystyle A\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K}) y \\displaystyle E_{ij}\\in\\mathfrak{M}_{n\\times p}(\\mathbb{K}) , \\displaystyle AE_{ij} es la matriz que en la columna \\displaystyle j tiene la columna \\displaystyle i de \\displaystyle A , y en el resto ceros. Caso particular, \\displaystyle p=1 . 2.​Dados \\displaystyle A\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K}) y \\displaystyle E_{ij}\\in\\mathfrak{M}_{p\\times m}(\\mathbb{K}) , \\displaystyle E_{ij}A es la matriz que en la fila \\displaystyle i tiene la fila \\displaystyle j de \\displaystyle A , y en el resto ceros. Caso particular, \\displaystyle p=1 . 3.​Si \\displaystyle A\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K}) es tal que \\displaystyle AX=0 para toda \\displaystyle X\\in\\mathfrak{M}_{n\\times p}(\\mathbb{K}) , entonces \\displaystyle A=0 . Análogamente, si \\displaystyle YA=0 para toda \\displaystyle Y\\in\\mathfrak{M}_{p\\times m}(\\mathbb{K}) , entonces \\displaystyle A=0 Demostración. 1. \\displaystyle AE_{ij}=\\begin{pmatrix}a_{11}&\\cdots&a_{1n}\\\\ \\vdots&\\ddots&\\vdots\\\\ a_{m1}&\\cdots&a_{mn}\\\\ \\end{pmatrix}\\cdot E_{ij}=\\begin{pmatrix}0&\\cdots&a_{1i}&\\cdots&0\\\\ 0&\\cdots&a_{2i}&\\cdots&0\\\\ \\vdots&&\\vdots&&\\\\ 0&\\cdots&a_{mi}&\\cdots&0\\\\ \\end{pmatrix} 2. \\displaystyle E_{ij}A=E_{ij}\\begin{pmatrix}a_{11}&\\cdots&a_{1n}\\\\ \\vdots&\\ddots&\\vdots\\\\ a_{m1}&\\cdots&a_{mn}\\\\ \\end{pmatrix}=\\begin{pmatrix}0&\\cdots&0\\\\ \\vdots&&\\vdots\\\\ a_{j1}&\\cdots&a_{jn}\\\\ \\vdots&&\\vdots\\\\ 0&\\cdots&0\\\\ \\end{pmatrix} 3.​Si \\displaystyle AX=0 para todo \\displaystyle X\\in\\mathfrak{M}_{n\\times p}(\\mathbb{K}) entonces \\displaystyle AE_{ij}=0 para todo elemento \\displaystyle E_{ij} de \\displaystyle\\mathfrak{{M}}_{n\\times p}(\\mathbb{K}) . Por tanto, todas las columnas de \\displaystyle A son cero, de donde \\displaystyle A es cero. De modo similar, si \\displaystyle YA=0 para toda \\displaystyle Y\\in\\mathfrak{{M}}_{p\\times m}(\\mathbb{K}) entonces \\displaystyle E_{ij}A=0 para todo \\displaystyle i,j , de donde se deduce que todas las filas de \\displaystyle A son cero. ∎ Proposición 2.4. \\displaystyle(\\mathfrak{M}_{n}(\\mathbb{K}),+,\\cdot) es un anillo con identidad, no conmutativo en general. Definición 2.9 (Inversa de una matriz). Decimos que una matriz cuadrada \\displaystyle A es invertible o regular si tiene inverso en el anillo de las matrices, es decir, si existe una matriz \\displaystyle B del mismo tamaño tal que \\displaystyle AB=BA=I_{n} . La matriz \\displaystyle B se dice inversa de \\displaystyle A . Si \\displaystyle A no es invertible, se dice singular. Proposición 2.5. 1.​Si \\displaystyle A\\in\\mathfrak{M}_{n}(\\mathbb{K}) es invertible, entonces la inversa de \\displaystyle A es unica y se denota \\displaystyle A^{-1} . 2.​Si \\displaystyle A es invertible y \\displaystyle B es tal que \\displaystyle AB=I_{n} , entonces \\displaystyle B=A^{-1} 3.​Si \\displaystyle A es invertible y \\displaystyle C es tal que \\displaystyle CA=I_{n} , entonces \\displaystyle C=A^{-1} . 4.​Si \\displaystyle A es invertible entonces \\displaystyle A^{-1} tambien es invertible y su inversa es \\displaystyle A . 5.​Si \\displaystyle A,B\\in\\mathfrak{M}_{n}(\\mathbb{K}) son matrices inversibles, entonces \\displaystyle AB tambien es invertible, y su inversa es \\displaystyle B^{-1}A^{-1} . Demostración. 1.​Si \\displaystyle B_{1},B_{2} son inversas de \\displaystyle A entonces \\displaystyle B_{1}=B_{1}I_{n}=B_{1}(AB_{2})=(B_{1}A)B_{2}=I_{N}B_{2}=B_{2} 2.​Si \\displaystyle AB=I_{n} , entonces \\displaystyle A^{-1}=A^{-1}\\cdot I_{n}=A^{-1}\\cdot(AB)=(A^{-1}A)B=I_{n}B% \\Rightarrow A^{-1}=B 3.​Analogo a la propiedad 2. 4.​Trivial. 5.​ \\displaystyle(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=AI_{n}A^{-1}=AA^{-1}=I_{n} . ∎ Definición 2.10 (Matrices elementales). Se llama matriz elemental a toda matriz cuadrada de orden \\displaystyle n de uno de los siguientes tipos: 1.​ \\displaystyle P_{ij}=I_{n}-E_{ii}+E_{ii}+E_{ji} 2.​ \\displaystyle P_{ij}(t)=I_{n}+tE_{ij}, con \\displaystyle t\\in\\mathbb{K} 3.​ \\displaystyle Q_{i}(s)=I_{n}+(s-1)E_{ii} , con \\displaystyle 0\\neq s\\in\\mathbb{K} donde \\displaystyle E_{ij} es la matriz cuadrada de orden \\displaystyle n que tiene todas sus entradas nulas salvo la entrada de la posicion \\displaystyle(ij) que es \\displaystyle 1_{K} . Ejemplo. \\displaystyle P_{12}=\\begin{pmatrix}0&1&0\\\\ 1&0&0\\\\ 0&0&1\\\\ \\end{pmatrix},\\quad P_{31}(t)=\\begin{pmatrix}1&0&0\\\\ 0&1&0\\\\ t&0&1\\\\ \\end{pmatrix},\\quad Q_{3}(s)=\\begin{pmatrix}1&0&0\\\\ 0&1&0\\\\ 0&0&s\\\\ \\end{pmatrix} Proposición 2.6. 1.​Para toda \\displaystyle A si \\displaystyle P_{ij},P_{ij}(t),Q_{i}(s) , \\displaystyle AP_{ij} se obtiene intercambiando las columnas \\displaystyle i,j de A. \\displaystyle AP_{ij}(t) se obtiene sumando a la columna \\displaystyle j de \\displaystyle A , la columna \\displaystyle i de \\displaystyle A multiplicada por \\displaystyle t . \\displaystyle AQ_{i}(s) se obtiene multiplicando la columna \\displaystyle i de \\displaystyle A por \\displaystyle s . 2.​Para toda \\displaystyle A si \\displaystyle P_{ij},P_{ij}(t),Q_{i}(s) , \\displaystyle P_{ij}A se obtiene intercambiando las filas \\displaystyle i,j de A. \\displaystyle P_{ij}(t)A se obtiene sumando a la fila \\displaystyle j de \\displaystyle A , la fila \\displaystyle i de \\displaystyle A multiplicada por \\displaystyle t . \\displaystyle Q_{i}(s)A se obtiene multiplicando la fila \\displaystyle i de \\displaystyle A por \\displaystyle s . 3.​Las matrices elementales son inversibles: \\displaystyle P_{ij}^{-1}=P_{ij} , \\displaystyle P_{ij}(t)^{-1}=P_{ij}(-t) y \\displaystyle Q_{i}(s)^{-1}=Q_{i}(s^{-1}) Proposición 2.7 (Propiedades de la trasposicion de matrices). Las propiedades de las trasposiciones de matrices son las siguientes: 1.​ \\displaystyle(A+B)^{t}=A^{t}+B^{t} 2.​ \\displaystyle(sA)^{t}=sA^{t} 3.​ \\displaystyle(AB)^{t}=B^{t}A^{t} 4.​ \\displaystyle A es una matriz invertible si y solo si \\displaystyle A^{t} tambien es invertible (y su inversa es \\displaystyle(A^{-1})^{t} ) 5.​ \\displaystyle E^{t}_{ij}=E_{ji},P^{t}_{ij}=P_{ij},Q_{i}(s)^{t}=Q_{i}(s) Previous page Next page"],[["index.html","Pt1.html","S3.html"],"3 Sistemas de ecuaciones lineales ‣ Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss ‣ Álgebra lineal‣ Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss ‣ Álgebra lineal","Skip to content. Sistemas de ecuaciones lineales 3 Sistemas de ecuaciones lineales Definición 3.1 (Sistema de ecuaciones lineales). Sea \\displaystyle\\mathbb{K} un cuerpo. Un sistema lineal de \\displaystyle m ecuaciones con \\displaystyle n incognitas es una expresion del tipo \\displaystyle\\begin{cases}a_{11}x_{1}+\\cdots+a_{1n}x_{n}=b_{1}\\\\ a_{21}x_{1}+\\cdots+a_{2n}x_{n}=b_{2}\\\\ \\vdots\\\\ a_{m1}x_{1}+\\cdots+a_{mn}x_{n}=b_{m}\\end{cases} donde todos los \\displaystyle a_{ij},b_{i}\\in\\mathbb{K} , \\displaystyle i=1,\\ldots,n,j=1,\\ldots,m .  ​los elementos \\displaystyle a_{ij} se llaman coeficientes del sistema.  ​los \\displaystyle x_{1},\\ldots,x_{n} se llaman incognitas  ​los \\displaystyle b_{1},\\ldots,b_{m} son los terminos independientes  ​cada una de las expresiones \\displaystyle a_{i1}x_{1}+\\cdots+a_{in}x_{n}=b_{i} se llama ecuacion del sistema. Los datos anteriores se pueden organizar utilizando matrices:  ​La matriz de tamaño \\displaystyle m\\times n formada por los coeficientes se llama la matriz del sistema: \\displaystyle A=\\begin{pmatrix}a_{11}&&a_{1n}\\\\ &\\ddots&\\\\ a_{m1}&&a_{mn}\\\\ \\end{pmatrix}\\in\\mathfrak{M}_{m\\times n}(\\mathbb{K})  ​La matriz columna formada por los terminos independientes se llama matriz de terminos independientes: \\displaystyle\\mathbf{b}=\\begin{pmatrix}b_{1}\\\\ \\vdots\\\\ b_{m}\\\\ \\end{pmatrix}\\in\\mathfrak{{M}}_{m\\times 1}(\\mathbb{K})  ​La matriz columna formada por las incognitas se llama matriz de incognitas: \\displaystyle\\mathbf{x}=\\begin{pmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\\\ \\end{pmatrix}  ​La matriz de tamaño \\displaystyle m\\times(n+1) formada por la matriz de coeficientes y una columna extra que contiene los terminos independientes se llama matriz ampliada: \\displaystyle(A\\mid\\mathbf{b})=\\left(\\begin{array}[]{ccc|c}a_{11}&&a_{1n}&b_{1% }\\\\ &\\ddots&&\\vdots\\\\ a_{m1}&&a_{mn}&b_{m}\\end{array}\\right)\\in\\mathfrak{{M}}_{m\\times(n+1)}(\\mathbb% {K}) Utilizando el producto de matrices, se puede comprobar que el sistema se puede reescribir de un modo mas compacto como \\displaystyle A\\cdot\\mathbf{x}=\\mathbf{b} Definición 3.2 (Soluciones de un sistema de ecuaciones lineales). Decimos que el vector columna \\displaystyle c=\\begin{pmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\\\ \\end{pmatrix} es una solucion del sistema \\displaystyle A\\cdot x=b si se cumple \\displaystyle A\\cdot\\mathbf{c}=\\mathbf{b} Es decir, una solucion del sistema son valores \\displaystyle c_{1},\\ldots,c_{n}\\in\\mathbb{K} que al sustituir \\displaystyle x_{1} por \\displaystyle c_{1} , etc. en la expresion hacen que las ecuaciones se conviertan en igualdades en K. Según su numero de soluciones, los sistemas se clasifican en:  ​Sistemas incompatibles: no tienen ninguna solución.  ​Sistemas compatibles: tienen soluciones •​Si solo admiten una solución, se llaman sistemas compatibles determinados. •​En caso contrario, se llaman sistemas compatibles indeterminados. Un sistema es homogeneo si todos sus terminos independientes son 0. Estos sistemas siempre son sistemas compatibles porque admiten como solucion al vector \\displaystyle 0=\\begin{pmatrix}0\\\\ \\vdots\\\\ 0\\\\ \\end{pmatrix} Definición 3.3. Se dice que dos sistemas \\displaystyle A\\cdot x=b y \\displaystyle B\\cdot x=c son equivalentes si tienen exactamente las mismas soluciones. Proposición 3.1. Si en un sistema de ecuaciones lineales 1.​se intercambian dos ecuaciones, 2.​se multiplica una ecuacion por un escalar no nulo, o 3.​se suma a una ecuacion otra multiplicada por un escalar, el sistema resultante es equivalente al original. Demostración. 1.​Trivial. 2.​Supongamos que multiplicamos por \\displaystyle\\alpha\\in\\mathbb{K}\\neq 0 la i-esima ecuacion del sistema \\displaystyle a_{i1}x_{1}+\\cdots+a_{in}x_{n}=b_{1} y que las demas ecuaciones las dejamos igual. Veamos que \\displaystyle c=(c_{1}\\,c_{2}\\,\\cdots\\,c_{n}) es solucion del primer sistema si y solo si es solucion del segundo: como hay una ecuacion diferente, basta comprobar que \\displaystyle c cumple la ecuacion \\displaystyle a_{i1}x_{1}+\\dots+a_{in}x_{n}=b_{i} si y solo si cumple la ecuacion \\displaystyle\\alpha a_{i1}x_{1}+\\dots+\\alpha a_{in}x_{n}=b_{i} \\displaystyle\\Rightarrow) Por cumplir el primer sistema, sabemos \\displaystyle a_{i1}c_{1}+\\cdots+a_{in}c_{n}=b_{i} y, multiplicando por \\displaystyle\\alpha en ambos lados de la igualdad, tenemos que \\displaystyle\\alpha a_{i1}c_{1}+\\cdots+\\alpha a_{in}c_{n}=\\alpha b_{i} Luego cumple la ecuacion \\displaystyle i del segundo sistema. Esto implica que \\displaystyle(c_{1}\\,\\cdots\\,c_{n}) es solucion del segundo sistema. \\displaystyle\\Leftarrow) Una solucion que cumple las ecuaciones del segundo sistema es \\displaystyle(c_{1}\\,\\cdots\\,c_{n}) . Por cumplir el segundo sistema, sabemos que \\displaystyle\\alpha a_{i1}x_{1}+\\cdots+\\alpha a_{in}x_{n}=b_{i} y multiplicando por \\displaystyle\\frac{1}{\\alpha} llegamos a \\displaystyle a_{i1}x_{1}+\\cdots+a_{in}x_{n}=b_{i} que es la ecuacion i-esima del primer sistema. Por tanto, \\displaystyle(c_{1}\\,\\cdots\\,c_{n}) tambien es solucion del primer sistema. 3.​Supongamos que la i-esima y la j-esima ecuaciones del sistema original son \\displaystyle\\begin{dcases}a_{i1}x_{1}+\\cdots+a_{in}x_{n}=b_{i}\\\\ a_{j1}x_{1}+\\cdots+a_{jn}x_{n}=b_{j}\\end{dcases} y sustituimos la ecuacion j-esima por la suma de la j-esima mas la i-esima multiplicada por un escalar \\displaystyle\\lambda\\in\\mathbb{K} : \\displaystyle\\begin{dcases}a_{i1}x_{1}+\\cdots+a_{in}x_{n}=b_{i}\\\\ (a_{j1}+\\lambda a_{i1})x_{1}+\\cdots+(a_{jn}+\\lambda a_{in})x_{n}=b_{j}+\\lambda b% _{i}\\end{dcases} \\displaystyle\\Rightarrow) Supongamos que \\displaystyle(c_{1}\\,\\cdots\\,c_{n}) es solucion del primer sistema. En particular, en la i-esima ecuacion \\displaystyle a_{i1}c_{1}+\\cdots+a_{in}c_{n}=b_{i} y en la j-esima \\displaystyle a_{j1}x_{1}+\\cdots+a_{jn}x_{n}=b_{i} Multiplicando la i-esima ecuacion por \\displaystyle\\lambda\\in\\mathbb{K} queda: \\displaystyle\\lambda a_{i1}c_{1}+\\cdots+\\lambda a_{in}c_{n}=\\lambda b_{i} que lo sumamos a la j-esima ecuacion \\displaystyle(a_{j1}+\\lambda a_{i1})c_{1}+\\cdots+(a_{jn}+\\lambda a_{in})c_{n}=% b_{j}+\\lambda b_{i} Llegamos a que se cumple la ecuacion j del segundo sistema y, por tanto, se cumplen todas las ecuaciones. \\displaystyle c_{1}\\,\\cdots\\,c_{n} es solucion del segundo sistema. \\displaystyle\\Leftarrow) Supongamos que \\displaystyle(c_{1}\\,\\cdots\\,c_{n}) es una solucion del segundo sistema, cuya i-esima ecuacion es: \\displaystyle a_{i1}c_{1}+\\cdots+a_{in}c_{n}=b_{i} y su j-esima: \\displaystyle(a_{j1}+\\lambda a_{i1})c_{1}+\\cdots+(a_{jn}+\\lambda a_{in})c_{n}=% b_{j}+\\lambda b_{i} Multiplicando en ambos lados de la i-esima ecuacion por \\displaystyle-\\lambda tenemos \\displaystyle-\\lambda a_{i1}c_{1}+\\cdots+-\\lambda a_{in}c_{n}=-\\lambda b_{i} Sumando a la j-esima ecuacion la anterior: \\displaystyle a_{j1}x_{1}+\\cdots+a_{jn}x_{n}=b_{i} Por tanto, \\displaystyle(c_{1}\\,\\cdots\\,c_{n}) cumple todas las ecuaciones del primer sistema y, por tanto, es solucion de ambos. Los sistemas son equivalentes. ∎ Definición 3.4 (Matrices equivalentes por filas). Dos matrices son equivalentes por filas si una se puede obtener a partir de la otra multiplicando por delante por una cantidad finita matrices elementales. Como las matrices elementales son todas invertibles, \\displaystyle B es equivalente por filas a \\displaystyle A si y solo si \\displaystyle A es equivalente por filas a \\displaystyle B . Tienen una relacion de equivalencia ya que cumplen las propiedades reflexiva, transitiva y simetrica. Definición 3.5 (Matriz escalonada). Una matriz \\displaystyle A esta en forma escalonada si cada fila no nula de \\displaystyle A comienza con mas ceros que la fila anterior y sus filas nulas, si las tiene, estan en la parte inferior de la matriz. Al primer elemento no nulo de cada fila no nula se le llama pivote de esa fila. Definición 3.6 (Matriz escalonada reducida). Una matriz \\displaystyle A esta en forma escalonada reducida si, ademas de ser escalonada, el pivote de cada fila no nula es 1 y los elementos superiores al pivote en su misma columna son cero. Cuando la matriz de coeficientes de un sistema \\displaystyle Ax=b esta en forma escalonada, la discusion y resolucion del sistema es casi directa. Los diferentes casos son: 1.​La ultima fila no nula de la matriz ampliada es de la forma \\displaystyle\\left(\\begin{array}[]{ccc|c}0&\\cdots&0&\\lambda\\\\ \\end{array}\\right) con \\displaystyle\\lambda\\neq 0\\in\\mathbb{K} . En este caso, el sistema es incompatible. 2.​La ultima fila no nula de la matriz ampliada es de la forma \\displaystyle\\left(\\begin{array}[]{ccc|c}0&\\cdots&\\lambda&\\alpha\\\\ \\end{array}\\right) con \\displaystyle\\lambda\\neq 0 . En este caso el sistema es compatible. Para resolverlo, se despejan “de abajo a arriba” las incógnitas que corresponden a los pivotes de cada fila no nula. Las incógnitas que no correspondan a pivotes de ninguna fila serán parámetros libres en la solución del sistema. Para decidir si es determinado o indeterminado, nos fijamos en el numero de pivotes y comparamos con el numero de incognitas. a)​Si hay tantas incognitas como pivotes, el sistema es compatible determinado. b)​Si hay mas incognitas que pivotes (hay filas no nulas) el sistema es compatible indeterminado, y va a depender exactamente del numero de incognitas que no corresponden a pivotes. Proposición 3.2 (Método de Gauss). Sea \\displaystyle A\\mathbf{x}=\\mathbf{b} un sistema de ecuaciones lineales. El metodo de Gauss transforma el sistema original en un sistema equivalente cuya matriz del sistema está en forma escalonada: 1.​Conseguir que en la primera fila el pivote esté lo mas a la izquierda posible. 2.​Hacer ceros debajo del pivote: se suma a cada una de las filas la primera multiplicada por el escalar adecuado. 3.​Repetir todos los pasos para las siguientes filas y columnas. Proposición 3.3 (Método de Gauss-Jordan). Sea \\displaystyle Ax=b un sistema de ecuaciones lineales. El método de Gauss-Jordan transforma el sistema original en un sistema equivalente cuya matriz del sistema está en formada escalonada reducida: 1.​Aplicar el metodo de Gauss, obteniendo la matriz de coeficientes en forma escalonada. 2.​Hacer que todos los pivotes sean iguales a \\displaystyle 1 multiplicando cada fila por un escalar. 3.​De abajo a arriba hacer ceros encima de cada pivote: se suma a cada una de las filas encima del pivote la última fila multiplicada por el escalar adecuado. Calculo de la inversa de una matriz A: Si \\displaystyle A tiene inversa y \\displaystyle A\\cdot C=I entonces \\displaystyle C=A^{-1}=\\left(\\begin{array}[]{c|c|c|c}C^{1}&C^{2}&\\cdots&C^{n}% \\\\ \\end{array}\\right) , donde \\displaystyle C^{1} , \\displaystyle\\ldots,C^{n} son las columnas de \\displaystyle A^{-1} . Por tanto, buscamos una matriz \\displaystyle C tal que \\displaystyle AC=I . \\displaystyle AC^{1}=\\begin{pmatrix}1\\\\ \\vdots\\\\ 0\\\\ \\end{pmatrix},C^{1}\\text{ solucion de }A\\mathbf{x}_{1}=\\begin{pmatrix}1\\\\ \\vdots\\\\ 0\\\\ \\end{pmatrix} \\displaystyle\\vdots \\displaystyle AC^{n}=\\begin{pmatrix}0\\\\ \\vdots\\\\ 1\\\\ \\end{pmatrix},C^{n}\\text{ solucion de }A\\mathbf{x}_{n}=\\begin{pmatrix}0\\\\ \\vdots\\\\ 1\\\\ \\end{pmatrix} Creamos una matriz ampliada con \\displaystyle n columnas nuevas, resolviendo todos los sistemas a la vez: \\displaystyle\\left(A\\left|\\begin{matrix}1&\\cdots&0\\\\ \\vdots&\\ddots&\\vdots\\\\ 0&\\cdots&1\\\\ \\end{matrix}\\right.\\right)=\\left(A\\left|I_{n}\\right.\\right) Hay dos casos:  ​Sistema incompatible: si en alguno de los pasos del método de Gauss sobre la matriz se obtiene una fila de la forma \\displaystyle\\begin{pmatrix}0&\\cdots&0&|\\cdots&x\\neq 0&\\cdots\\\\ \\end{pmatrix}\\Rightarrow\\text{No existe }A^{-1}  ​En caso contrario, la matriz escalonada reducida asociada a \\displaystyle A es \\displaystyle I_{n} y el método de Gauss-Jordan aplicado a \\displaystyle(A|I_{n}) nos proporciona una matriz de la forma \\displaystyle(I_{n}|C) , con \\displaystyle C la inversa de \\displaystyle A . \\displaystyle(A|I_{n})\\sim_{\\text{Gauss-Jordan}}(I_{n}|A^{-1}) Proposición 3.4. Una matriz \\displaystyle A es invertible si y solo si \\displaystyle A es producto de matrices elementales. Demostración. \\displaystyle\\Rightarrow) Si \\displaystyle A es invertible, el metodo de Gauss-Jordan sobre \\displaystyle(A\\mid I) nos devuelve \\displaystyle(I\\mid A^{-1}) . Cada paso del metodo de Gauss-Jordan es una operacion elemental que implica multiplicar \\displaystyle A por una matriz elemental: \\displaystyle\\underbrace{E_{x}\\cdots E_{1}}_{\\begin{subarray}{c}matrices\\\\ elementales\\end{subarray}}\\cdot(A\\mid I)=(I\\mid A^{-1}) Como las matrices son invertibles: \\displaystyle E_{k}\\cdots E_{1}\\cdot A=I\\Rightarrow A=E^{-1}_{1}\\cdots E^{-1}_% {k} Asi, tenemos que \\displaystyle A es producto de matrices elementales (las inversas de matrices elementales son otras matrices elementales). \\displaystyle\\Leftarrow) Supongamos que \\displaystyle A=E_{1}\\cdot E_{2}\\cdots E_{k} para ciertas matrices elementales. Por tanto, \\displaystyle A es invertible y su inversa es \\displaystyle A^{-1}=(E_{1}\\cdots E_{k})^{-1}=E^{-1}_{n}\\cdots E^{-1}_{1} (las matrices elementales son invertibles). ∎ Proposición 3.5. Dos matrices \\displaystyle A,B son invertibles si y solo si su producto \\displaystyle AB es invertible. Demostración. \\displaystyle\\Rightarrow) Ya esta demostrado. \\displaystyle\\Leftarrow) Lo probaremos por reduccion al absurdo. Supongamos que \\displaystyle A no tiene inversa. Como no es invertible, al usar Gauss-Jordan sobre \\displaystyle(A\\mid I) aparecera una fila de ceros que haga el sistema un sistema incompatible. Es decir, habran matrices elementales que cumplan \\displaystyle E_{1},\\ldots,E_{k} tales que en el producto \\displaystyle E_{k}\\cdots E_{1}\\cdot A aparece una fila de ceros. Multiplicando por B a la derecha: \\displaystyle E_{k}\\cdots E_{1}\\cdot A\\cdot B=\\begin{pmatrix}&\\cdots&\\\\ 0&\\cdots&0\\\\ &\\cdots&\\\\ \\end{pmatrix}\\cdot B=\\begin{pmatrix}&\\cdots&\\\\ 0&\\cdots&0\\\\ &\\cdots&\\\\ \\end{pmatrix} Esto es una contradiccion con que \\displaystyle A\\cdot B es invertible. Por lo tanto, \\displaystyle A tiene que ser invertible. Tenemos que \\displaystyle AB es invertible y hemos visto que \\displaystyle A tambien lo es. Queda demostrar que \\displaystyle B es invertible. Teniendo en cuenta que el producto de invertibles es invertible, llegamos a \\displaystyle(A^{-1})(A\\cdot B)=B\\Rightarrow\\text{es invertible. } ∎ Previous page Next page"],[["index.html","Pt1.html","S4.html"],"4 Determinantes de matrices cuadradas ‣ Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss ‣ Álgebra lineal‣ Parte I Matrices y determinantes. Sistemas de ecuaciones lineales, método de Gauss ‣ Álgebra lineal","Skip to content. Determinantes de matrices cuadradas 4 Determinantes de matrices cuadradas Definición 4.1 (Determinante). Por inducción en el tamaño de \\displaystyle A\\in\\mathfrak{M}_{n}(\\mathbb{K}) .  ​Si \\displaystyle a=1 : \\displaystyle\\det((a_{11}))=a_{11} .  ​Supongamos que sabemos calcular determinantes de matrices de orden \\displaystyle n-1 y sea \\displaystyle A=(a_{ij}\\in\\mathfrak{M}_{n}(\\mathbb{K})) . Llamamos \\displaystyle(ij)-esimo adjunto de la matriz \\displaystyle A al numero \\displaystyle\\alpha_{ij}=(-1)^{i+1}\\det(A_{ij}) donde \\displaystyle A_{ij} es la matriz de orden \\displaystyle n-1 obtenida eliminando la fila \\displaystyle i y la columna \\displaystyle j de \\displaystyle A . Definimos el determinante de \\displaystyle A como \\displaystyle\\det(A)=a_{11}\\alpha_{11}+a_{21}\\alpha_{21}+\\cdots+a_{n1}\\alpha_{% n1} A esta formula se le llama desarrollo del determinante por adjuntos de la primera columna. Lema 4.1. El determinante de la matriz identidad es igual a \\displaystyle 1 . Demostración. Por induccion en el orden \\displaystyle n de la matriz identidad.  ​Si \\displaystyle n=1 , \\displaystyle\\det(I_{1})=\\det(1)=1 .  ​Supongamos que \\displaystyle\\det(I_{n-1})=1 y veamos que \\displaystyle\\det(I_{n})=1 . Por una parte, el adjunto \\displaystyle(11) -esimo de \\displaystyle I_{n} es \\displaystyle\\alpha_{11}=(-1)^{1+1}\\det(I_{n-1}) , asi que por la hipotesis de induccion, \\displaystyle\\alpha_{11}=1 . Ademas, las entradas \\displaystyle(21),(31),\\ldots,(n1) de la matriz identidad son todas iguales a cero, asi que no hace falta calcular \\displaystyle\\alpha_{21},\\ldots,\\alpha_{n1} . Sustituyendo en la formula del desarrollo del determinante por adjuntos de la primera columna \\displaystyle\\det(I_{n})=1\\alpha_{11}+0\\alpha_{21}+\\cdots+0\\alpha_{n1}=1. ∎ Proposición 4.1 (Propiedades de los determinantes). 1.​Si \\displaystyle A,A^{\\prime},A^{\\prime\\prime} son tres matrices identicas de orden \\displaystyle n salvo en que la fila \\displaystyle i de \\displaystyle A es la suma de la fila \\displaystyle i de \\displaystyle A^{\\prime} y la fila \\displaystyle i de \\displaystyle A^{\\prime\\prime} entonces \\displaystyle\\det(A)=\\det(A^{\\prime})+\\det(A^{\\prime\\prime}) 2.​Si dos filas de \\displaystyle A son iguales, entonces \\displaystyle\\det(A)=0 . 3.​Si se intercambian dos filas de \\displaystyle A , entonces el determinante cambia de signo. 4.​Si multiplicamos una fila de \\displaystyle A por un escalar \\displaystyle\\lambda\\in K entonces el determinante de la matriz obtenida es igual a \\displaystyle\\lambda\\det(A) . 5.​El determinante de \\displaystyle A no varia si a una fila de \\displaystyle A le sumamos otra fila multiplicada por un escalar. 6.​ \\displaystyle A es invertible si y solo si \\displaystyle\\det(A)\\neq 0 . 7.​ \\displaystyle\\det(A\\cdot B)=\\det(A)\\cdot\\det(B) . 8.​ \\displaystyle\\det(A)=\\det(A^{t}) . En particular, las propiedades \\displaystyle(1)-(5) anteriores se cumplen tambien si se cambia la palabra “fila” por la palabra “columna”. 9.​Se puede desarrollar el determinante por cualquier columna o cualquier fila de \\displaystyle A . Demostración. Las propiedades 1), 2) y 4) se demuestran por inducción. Veamos la demostración del resto de propiedades: 3.​Veamos que esta propiedad se deduce de 1) y 2). Expresamos \\displaystyle A por filas: \\displaystyle A=\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right) y construimos la matriz auxiliar \\displaystyle A^{\\prime}=\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{i}+A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}+A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right) Entonces, por 1) y 2), \\displaystyle 0=_{1)}\\det(A^{\\prime})=_{2)}\\underbrace{\\det\\left(\\begin{array}% []{c}\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right)}_{=_{1)}0}+\\det\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right)+\\det\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right)+\\underbrace{\\det\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right)}_{=_{1)}0} de donde \\displaystyle\\det(A)=\\det\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right)=-\\det\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right) 5.​Si expresamos \\displaystyle A por filas \\displaystyle A=\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right), tomamos \\displaystyle\\lambda\\in\\mathbb{K} y vamos a calcular el determinante de la matriz que en la fila \\displaystyle i tiene la fila \\displaystyle i de \\displaystyle A mas \\displaystyle\\lambda por la fila \\displaystyle j : \\displaystyle\\det\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{i}+\\lambda A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right)=_{1),4)}\\det\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right)+\\lambda\\det\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right)=_{2)}\\det\\left(\\begin{array}[]{c}\\vdots\\\\ \\hline\\cr A_{i}\\\\ \\hline\\cr\\vdots\\\\ \\hline\\cr A_{j}\\\\ \\hline\\cr\\vdots\\\\ \\end{array}\\right) 6.​Utilizando las propiedades anteriores y que \\displaystyle\\det(I_{n})=1 , vamos a calcular cuánto valen los determinantes de las matrices elementales \\displaystyle P_{ij},P_{ij}(t)(i\\neq j) y \\displaystyle Q_{i}(s) :  ​Como \\displaystyle P_{ij} , \\displaystyle i\\neq j , multiplicada por delante de la matriz identidad, intercambia las filas \\displaystyle i y \\displaystyle j de la matriz identidad, \\displaystyle-1=_{3)}\\det(P_{ij}I_{n})=\\det(P_{ij}).  ​Como \\displaystyle P_{ij}(\\lambda) , multiplicada por delante de la matriz identidad, suma a la fila \\displaystyle i la fila \\displaystyle j de la matriz identidad multiplicada por \\displaystyle\\lambda , y eso no afecta al valor del determinante, \\displaystyle 1=_{4)}\\det(P_{ij}(\\lambda)I_{n})=\\det(P_{ij}(\\lambda)).  ​Como \\displaystyle Q_{i}(s) , multiplicada por delante de la matriz identidad, multiplica la fila \\displaystyle i de la matriz identidad por \\displaystyle s , el determinante de la matriz resultante es \\displaystyle s\\det(I_{n}) , asi que \\displaystyle s=_{5)}\\det(Q_{i}(s)I_{n})=\\det(Q_{i}(s)). Las propiedades 3), 4) y 5) se pueden escribir usando matrices elementales. Si \\displaystyle A es una matriz cualquiera, \\displaystyle i\\neq j ,  ​ \\displaystyle\\det(P_{ij}A)=_{3)}-\\det(A)=\\det(P_{ij})\\det(A)  ​ \\displaystyle\\det(P_{ij}A)=_{4)}\\det(A)=\\det(P_{ij}(\\lambda))\\det(A)  ​ \\displaystyle\\det(Q_{i}(s)A)=_{5)}s\\det(A)=\\det(Q_{i}(s))\\det(A) y llegamos a la concluson de que si \\displaystyle E denota cualquiera de las matrices elementales y \\displaystyle A es una matriz en general \\displaystyle\\det(EA)=\\det(E)\\det(A) Vamos a usar esta propiedad para probar que \\displaystyle A es invertible si y solo si el determinante de \\displaystyle A es no nulo. \\displaystyle\\Rightarrow) Sabemos que \\displaystyle A es invertible si y solo si \\displaystyle A=E_{1}\\cdot E_{2}\\cdots E_{k} para ciertas matrices elementales \\displaystyle E_{1},\\ldots,E_{k} . Por tanto, \\displaystyle\\det(A)=\\det(E_{1}\\cdot E_{2}\\cdots E_{k})=\\det(E_{1})\\det(E_{2}% \\cdots E_{k})=\\underbrace{\\det(E_{1})}_{\\neq 0}\\underbrace{\\det(E_{2})}_{\\neq 0% }\\cdots\\underbrace{\\det(E_{k})}_{\\neq 0}\\neq 0 \\displaystyle\\Leftarrow) Veamos que si \\displaystyle A no es invertible, entonces el determinante de \\displaystyle A es cero: si \\displaystyle A no es invertible, cuando apliquemos el método de Gauss-Jordan, que consiste en ir multiplicando \\displaystyle A por delante por matrices elementales hasta encontrar una matriz escalonada reducida, nos encontraremos con una matriz escalonada reducida \\displaystyle R que tiene toda una fila de ceros. Por la propiedad 4), si una matriz tiene toda una fila de ceros, su determinante es cero. Así \\displaystyle 0=_{4)}\\det(R)=\\det(E_{1}\\cdots E_{k}A)=\\underbrace{\\det(E_{1})}% _{\\neq 0}\\cdots\\underbrace{\\det(E_{k})}_{\\neq 0}\\det(A) de donde se obtiene que \\displaystyle\\det(A)=0 . 7.​Hasta ahora hemos probado que \\displaystyle\\det(EA)=\\det(E)\\det(A) , para \\displaystyle E una matriz elemental. Veamos que en general \\displaystyle\\det(AB)=\\det(A)\\det(B) , para \\displaystyle A y \\displaystyle B dos matrices cualquiera. Distinguimos dos casos: a)​Si \\displaystyle\\det(A)=0 o \\displaystyle\\det(B)=0 , entonces, por 6) \\displaystyle A o \\displaystyle B no son invertibles, con lo que \\displaystyle AB no es invertible y, de nuevo, por \\displaystyle 6) , \\displaystyle 0=\\det(AB)=\\det(A)\\det(B) b)​Si \\displaystyle\\det(A)\\neq 0 y \\displaystyle\\det(B)\\neq 0 , entonces, por \\displaystyle 6) , tanto \\displaystyle A como \\displaystyle B son invertibles (producto cada una de ellas por matrices elementales). Supongamos que \\displaystyle A=E_{1}\\cdots E_{k} , \\displaystyle B=E^{\\prime}_{1}\\cdots E^{\\prime}_{s} . Así, por la conclusión anterior, \\displaystyle\\det(AB) \\displaystyle=\\det(E_{1}\\cdots E_{k}E^{\\prime}_{1}\\cdots E^{\\prime}_{s}) \\displaystyle=\\det(E_{1})\\cdots\\det(E_{k})\\det(E^{\\prime}_{1})\\cdots\\det(E^{% \\prime}_{s}) \\displaystyle=\\det(A)\\det(B). 8.​Distinguimos dos casos: a)​Si \\displaystyle\\det(A)=0 , entonces, por \\displaystyle 6), \\displaystyle A no es invertible, con lo que \\displaystyle A^{t} tampoco es invertible y, por \\displaystyle 6) , \\displaystyle\\det(A^{t})=0 . b)​Si \\displaystyle\\det(A)\\neq 0 , entonces, por \\displaystyle 6) , \\displaystyle A es invertible ( \\displaystyle A es producto de matrices elementales \\displaystyle A=E_{1}\\cdots E_{k} ). Para cada matriz elemental, es fácil comprobar que \\displaystyle\\det(E)=\\det(E^{t}) , así que \\displaystyle\\det(A^{t}) \\displaystyle=\\det((E_{1}\\cdots E_{k})^{t})=\\det(E^{t}_{k}\\cdots E^{t}_{1}) \\displaystyle=\\det(E^{t}_{k})\\cdots\\det(E^{t}_{1})=\\det(E_{k})\\cdots\\det(E_{1}) \\displaystyle=\\det(E_{1})\\cdots\\det(E_{k})=\\det(E_{1}\\cdots E_{k}) \\displaystyle=\\det(A) 9.​Se sigue de 3) y 8). ∎ Previous page Next page"],[["index.html","Pt2.html","S5.html"],"5 Definición de espacio vectorial ‣ Parte II Espacios vectoriales ‣ Álgebra lineal‣ Parte II Espacios vectoriales ‣ Álgebra lineal","Skip to content. Definición de espacio vectorial 5 Definición de espacio vectorial Definición 5.1 (Espacio vectorial sobre un cuerpo \\displaystyle K ). Decimos que \\displaystyle V es un espacio vectorial sobre el cuerpo \\displaystyle\\mathbb{K} si \\displaystyle V es un conjunto dotado de dos operaciones \\displaystyle\\begin{aligned} +\\colon V\\times V&\\longrightarrow V\\\\ (v,w)&\\longmapsto v+w\\end{aligned}\\qquad\\begin{aligned} \\cdot_{K}\\colon K% \\times V&\\longrightarrow V\\\\ (\\alpha,v)&\\longmapsto\\alpha\\cdot_{K}v\\end{aligned} y se cumplen las siguientes propiedades: 1.​ \\displaystyle(V,+) es un grupo abeliano. 2.​ \\displaystyle\\alpha(v_{1}+v_{2})=\\alpha v_{1}+\\alpha v_{2} para todo \\displaystyle\\alpha\\in\\mathbb{K},v_{1},v_{2}\\in V 3.​ \\displaystyle(\\alpha+\\beta)v=\\alpha v+\\beta v para todo \\displaystyle\\alpha,\\beta\\in\\mathbb{K},v\\in V 4.​ \\displaystyle(\\alpha\\beta)v=\\alpha(\\beta v) para todo \\displaystyle\\alpha,\\beta\\in\\mathbb{K},v\\in V 5.​ \\displaystyle 1_{K}v=v para todo \\displaystyle v\\in V Los elementos de \\displaystyle V se llaman vectores y los elementos de \\displaystyle K se llaman escalares. Ejemplo. Veamos varios ejemplos de espacios vectoriales: 1.​Sea \\displaystyle\\mathbb{K} un cuerpo. \\displaystyle\\mathbb{K} es un espacio vectorial sobre \\displaystyle\\mathbb{K} con la suma y el producto por escalares de \\displaystyle\\mathbb{K} . 2.​Si \\displaystyle V es un espacio vectorial sobre un cuerpo \\displaystyle\\mathbb{K} y \\displaystyle\\hat{K} es subcuerpo de \\displaystyle\\mathbb{K} entonces \\displaystyle V también es un espacio vectorial sobre \\displaystyle\\hat{K} . Por ejemplo, como \\displaystyle\\mathbb{Q} es subcuerpo de \\displaystyle\\mathbb{R} y \\displaystyle\\mathbb{R} es subcuerpo de \\displaystyle\\mathbb{C} , \\displaystyle\\mathbb{C} es un espacio vectorial sobre \\displaystyle\\mathbb{C} \\displaystyle\\mathbb{C} es un espacio vectorial sobre \\displaystyle\\mathbb{R} y \\displaystyle\\mathbb{C} es un espacio vectorial sobre \\displaystyle\\mathbb{Q} . Además, estos tres espacios vectoriales son diferentes entre sí. 3.​Si \\displaystyle V y \\displaystyle W son dos espacios vectoriales sobre el mismo cuerpo \\displaystyle\\mathbb{K} , entonces el producto cartesiano \\displaystyle V\\times W=\\{(v,w)\\mid v\\in V,w\\in W\\} es un espacio vectorial sobre \\displaystyle K con las operaciones \\displaystyle(v_{1},w_{1})+(v_{2},w_{2})=(v_{1}+v_{2},w_{1}+w_{2}) y \\displaystyle\\alpha(v,w)=(\\alpha v,\\alpha w) para todo \\displaystyle v,v_{1},v_{2}\\in V , \\displaystyle w,w_{1},w_{2}\\in W , \\displaystyle\\alpha\\in\\mathbb{K} . En particular, para \\displaystyle n\\in\\mathbb{N} \\displaystyle\\mathbb{K}^{n}=\\{(x_{1},\\ldots,x_{n})\\mid x_{1},\\ldots,x_{n}\\in% \\mathbb{K}\\} es un espacio vectorial sobre \\displaystyle\\mathbb{K} . Por ejemplo, tenemos los siguientes espacios vectoriales: \\displaystyle\\mathbb{R}^{2},\\mathbb{Q}^{2},\\mathbb{R}^{7},\\mathbb{C}^{3}, etc. 4.​El conjunto de las matrices \\displaystyle\\mathfrak{{M}}_{m\\times n}(\\mathbb{K}) con la suma de matrices y el producto por escalares tiene estructura de espacio vectorial sobre \\displaystyle\\mathbb{K} . 5.​Si denotamos por \\displaystyle\\mathbb{K}[x] al conjunto de todos los polinomios en la variable \\displaystyle x con coeficientes en \\displaystyle\\mathbb{K} , \\displaystyle\\mathbb{K}[x] es un espacio vectorial sobre \\displaystyle\\mathbb{K} con la suma y el producto por escalares habituales. 6.​Si denotamos por \\displaystyle\\mathbb{K}_{n}[x] al conjunto de todos los polinomios de grado menor o igual que \\displaystyle n en la variable \\displaystyle x y con coeficientes en \\displaystyle\\mathbb{K} , \\displaystyle\\mathbb{K}_{n}[x] es un espacio vectorial sobre \\displaystyle\\mathbb{K} . 7.​Si \\displaystyle\\mathbb{K} es un cuerpo, las sucesiones de números de \\displaystyle\\mathbb{K} con la suma por componentes y el producto por escalares componente a componente forman un espacio vectorial sobre \\displaystyle\\mathbb{K} . Lema 5.1. Sea \\displaystyle V un espacio vectorial sobre un cuerpo \\displaystyle\\mathbb{K} . 1.​ \\displaystyle\\alpha 0_{V}=0_{V} para todo \\displaystyle\\alpha\\in\\mathbb{K} 2.​ \\displaystyle 0_{K}v=0_{V} para todo \\displaystyle v\\in V 3.​Si \\displaystyle\\alpha v=0_{V} entonces \\displaystyle\\alpha=0_{K} o \\displaystyle v=0_{V} 4.​ \\displaystyle(-1)v=-(\\alpha v)=-v para todo \\displaystyle v\\in V Demostración. 1. \\displaystyle\\alpha 0_{V}=\\alpha(0_{V}+0_{V})=_{A2)}\\alpha 0_{V}+\\alpha 0_{V} ​así que sumando el opuesto de \\displaystyle\\alpha 0_{V} en ambos lados de la igualdad llegamos a que \\displaystyle 0_{V}=0_{V}+\\alpha 0_{V}=\\alpha 0_{V} . 2. \\displaystyle 0_{K}v=(0_{K}+0_{K})v=_{A3)}0_{K}v+0_{K}v ​asi que sumando el opuesto de \\displaystyle 0_{K}v en ambos lados de la igualdad llegamos a que \\displaystyle 0_{V}=0_{V}+0_{K}v=0_{K}v . 3.​Supongamos que \\displaystyle\\alpha v=0_{V} y que \\displaystyle\\alpha\\neq 0_{K} . Como \\displaystyle\\mathbb{K} es un cuerpo y \\displaystyle\\alpha\\neq 0 , \\displaystyle v=_{A5)}1_{K}v=(\\frac{1}{\\alpha}\\alpha)v=_{A3)}\\frac{1}{\\alpha}(% \\alpha v)=\\frac{1}{\\alpha}0_{V}=_{1)}0_{V} 4.​Veamos que \\displaystyle(-1)v es el vector opuesto de \\displaystyle v : \\displaystyle v+(-1)v=_{A5)}1v+(-1)v=_{A3)}(1-1)v=0_{K}v=_{2)}0_{V} así que \\displaystyle(-1)v=-v . ∎ Previous page Next page"],[["index.html","Pt2.html","S6.html"],"6 Subespacios vectoriales ‣ Parte II Espacios vectoriales ‣ Álgebra lineal‣ Parte II Espacios vectoriales ‣ Álgebra lineal","Skip to content. Subespacios vectoriales 6 Subespacios vectoriales Definición 6.1 (Subespacio vectorial). Sea \\displaystyle V un espacio vectorial sobre un cuerpo \\displaystyle\\mathbb{K} . Decimos que un subconjunto no vacío \\displaystyle S de \\displaystyle V es un subespacio vectorial de \\displaystyle V si \\displaystyle S con la suma y el producto de \\displaystyle V tiene de nuevo estructura de espacio vectorial. Notacion: \\displaystyle S\\leq V . En particular, la suma de vectores de \\displaystyle S da vectores de \\displaystyle S y el producto por escalares de vectores de \\displaystyle S da vectores de \\displaystyle S :  ​ \\displaystyle v,w\\in S\\Rightarrow v+w\\in S ,  ​ \\displaystyle v\\in S,\\alpha\\in\\mathbb{K}\\Rightarrow\\alpha v\\in S . Proposición 6.1. Sea \\displaystyle V un espacio vectorial sobre un cuerpo \\displaystyle\\mathbb{K} . Las siguientes condiciones son equivalentes: 1.​ \\displaystyle\\varnothing\\neq S\\subset V es un subespacio vectorial de \\displaystyle V . 2.​ \\displaystyle S\\neq\\varnothing y \\displaystyle\\alpha v+\\beta w\\in S para todo \\displaystyle\\alpha,\\beta\\in\\mathbb{K},v,w\\in S 3.​Se cumplen las tres condiciones: a)​ \\displaystyle 0_{V}\\in S b)​ \\displaystyle v,w\\in S\\Rightarrow v+w\\in S c)​ \\displaystyle v\\in S,\\alpha\\in S\\Rightarrow\\alpha v\\in S Demostración. \\displaystyle 1)\\Rightarrow 2) Directo por la definicion. \\displaystyle 2)\\Rightarrow 3) Como \\displaystyle S\\neq\\varnothing existe al menos un vector \\displaystyle v\\in S . Tomando \\displaystyle\\alpha=1,\\beta=-1\\in\\mathbb{K} tenemos que \\displaystyle 0_{V}=1v+(-1)v\\in S Ademas, para todo \\displaystyle v,w\\in S , tomando \\displaystyle\\alpha=\\beta=1\\in\\mathbb{K} , tenemos \\displaystyle v+w\\in S , y tomando \\displaystyle\\alpha\\in\\mathbb{K} y \\displaystyle\\beta=0 tenemos \\displaystyle\\alpha v\\in S . \\displaystyle 3)\\Rightarrow 1) La condicion 1 asegura que \\displaystyle S\\neq\\varnothing . Ademas, las condiciones 2 y 3 aseguran que se pueden definir la suma de vectores y el producto por escalares en \\displaystyle S : \\displaystyle\\begin{aligned} +\\colon S\\times S&\\longrightarrow S\\\\ (v,w)&\\longmapsto v+w\\end{aligned}\\qquad\\begin{aligned} \\cdot_{K}\\colon K% \\times S&\\longrightarrow S\\\\ (\\alpha,v)&\\longmapsto\\alpha v\\end{aligned} Ademas los axiomas 1-5 se cumplen para vectores de \\displaystyle S porque se cumplen para vectores de \\displaystyle V y \\displaystyle S\\subset V . ∎ Cuando queramos comprobar si un subconjunto es o no subespacio vectorial de \\displaystyle V , usaremos normalmente la caracterizacion 3) de la proposicion anterior. Ejemplo. Veamos varios ejemplos de subconjuntos que son o no subespacios de los espacios vectoriales dados: 1.​ \\displaystyle W=\\mathbb{C} e.v. sobre \\displaystyle\\mathbb{R} . \\displaystyle S=\\mathbb{R}\\leq W ya que \\displaystyle 0\\in\\mathbb{R} , \\displaystyle v+w\\in\\mathbb{R}\\;\\forall v,w\\in\\mathbb{R} y \\displaystyle\\alpha\\cdot v\\in\\mathbb{R}\\;\\forall v\\in\\mathbb{R},\\forall a\\in% \\mathbb{R} . \\displaystyle V=\\mathbb{C} e.v. sobre \\displaystyle\\mathbb{C} . \\displaystyle S=\\mathbb{R} no es un subespacio vectorial de V. Contraejemplo: \\displaystyle v=1\\in S,\\alpha=i\\in\\mathbb{C}\\Rightarrow\\alpha\\cdot v=i\\notin S 2.​ \\displaystyle V=K^{3} e.v. sobre \\displaystyle\\mathbb{K} .  ​ \\displaystyle S_{1}=\\{(x,x,x)\\mid x\\in\\mathbb{K}\\} es un subespacio de V, demostrandolo con el mismo procedimiento.  ​ \\displaystyle S_{2}=\\{(x,0,0)\\mid x\\in\\mathbb{K}\\} es un subespacio vectorial de \\displaystyle V .  ​ \\displaystyle S_{3}=\\{(x,y,z)\\in\\mathbb{K}^{3}\\mid x+3y-2z=0\\} es subespacio vectorial de \\displaystyle V . •​ \\displaystyle(0,0,0)\\in S^{3} porque \\displaystyle 0+3\\cdot 0-2\\cdot 0=0 . •​Sean \\displaystyle(x_{1},y_{1},z_{1})\\in S_{3} y \\displaystyle(x_{2},y_{2},z_{2})\\in S_{3} . Sabemos que \\displaystyle\\begin{rcases}x_{1}+3y_{1}-2z_{1}=0\\\\ x_{2}+3y_{2}-2z_{2}=0\\end{rcases}\\Rightarrow(x_{1}+x_{2})+3(y_{1}+y_{2})-2(z_{% 1}+z_{2})=0 Si \\displaystyle(x,y,z)\\in S_{3} y \\displaystyle\\alpha\\in\\mathbb{K} , \\displaystyle x+3y-2z=0\\Rightarrow(\\alpha x)+3(\\alpha y)-2(\\alpha z)=0 . Es decir, \\displaystyle(\\alpha x,\\alpha y,\\alpha z)\\in S_{3} . 3.​Consideremos \\displaystyle V=\\mathfrak{M}_{2}(\\mathbb{K}) como espacio vectorial sobre \\displaystyle\\mathbb{K} . Entonces  ​ \\displaystyle S_{1}=\\{\\begin{pmatrix}a&b\\\\ c&0\\\\ \\end{pmatrix}\\mid a=b+c\\}=\\{\\begin{pmatrix}b+c&b\\\\ c&0\\\\ \\end{pmatrix}\\mid b,c\\in\\mathbb{K}\\} es un subespacio vectorial de \\displaystyle V .  ​ \\displaystyle S_{3}=\\{A\\in V\\mid A^{t}=A\\} es un subespacio vectorial de \\displaystyle V . Si \\displaystyle A=A^{t}yB=B^{t} , \\displaystyle(A+B)^{t}=A^{t}+B^{t}=A+B . Por otro lado, si \\displaystyle A=A^{t} y \\displaystyle\\alpha\\in\\mathbb{K} , entonces \\displaystyle(\\alpha A)^{t}=\\alpha A^{t}=\\alpha A . Esto tambien se cumple con el conjunto de matrices antisimétricas.  ​ \\displaystyle S_{5}=\\{A\\in V\\mid A^{t}=A\\text{ o }A^{t}=-A\\} no es subespacio vectorial de \\displaystyle V . Contraejemplo: \\displaystyle A=\\begin{pmatrix}1&2\\\\ 2&1\\\\ \\end{pmatrix}\\in S_{6} , \\displaystyle B=\\begin{pmatrix}0&1\\\\ -1&0\\\\ \\end{pmatrix}\\in S_{6} , pero \\displaystyle A+B=\\begin{pmatrix}1&3\\\\ 1&1\\\\ \\end{pmatrix} Proposición 6.2 (Interseccion y union de subespacios). Dada una colección de subespacios \\displaystyle S_{i} , \\displaystyle i\\in I , de un espacio vectorial \\displaystyle V , se puede comprobar que la intersección \\displaystyle\\bigcap_{i\\in I}S_{i}\\text{ siempre es un subespacio vectorial. } Sin embargo, la unión de subespacios vectoriales, en general, no es subespacio. Por ejemplo, si consideramos \\displaystyle V=\\mathbb{R}^{2} y los subespacios \\displaystyle S_{1}=\\{(x,0)\\mid x\\in\\mathbb{R}\\} y \\displaystyle S_{2}=\\{(0,y)\\mid y\\in\\mathbb{R}\\} , la union \\displaystyle S_{1}\\cup S_{2}=\\{(x,0)\\mid x\\in\\mathbb{R}\\}\\cup\\{(0,y)\\mid y\\in% \\mathbb{R}\\} no es subespacio vectorial: \\displaystyle\\begin{rcases}(1,0)\\in S_{1}\\cup S_{2}\\\\ (0,1)\\in S_{1}\\cup S_{2}\\end{rcases}\\text{ pero }(1,0)+(0,1)=(1,1)\\not\\in S_{1% }\\cup S_{2} El menor subespacio que contiene a dos subespacios dados es la suma de subespacios. Definición 6.2. Sea \\displaystyle V un espacio vectorial sobre \\displaystyle\\mathbb{K} y sean \\displaystyle S_{1},S_{2} dos subespacios de \\displaystyle V . Definimos la suma de los subespacios \\displaystyle S_{1} y \\displaystyle S_{2} y lo denotamos como \\displaystyle S_{1}+S_{2} del siguiente modo: \\displaystyle S_{1}+S_{2}=\\{v_{1}+v_{2}\\mid v_{1}\\in S_{1},v_{2}\\in S_{2}\\} Proposición 6.3. \\displaystyle S_{1}+S_{2} es subespacio vectorial de \\displaystyle V . Demostración. 1.​Como \\displaystyle 0_{V}\\in S_{1} y \\displaystyle 0_{V}\\in S_{2} , entones \\displaystyle 0_{V}=0_{V}+0_{V}\\in S_{1}+S_{2} 2.​Si \\displaystyle v_{1}+v_{2}\\in S_{1}+S_{2} y \\displaystyle w_{1}+w_{2}\\in S_{1}+S_{2} , donde \\displaystyle v_{1},w_{1}\\in S_{1} y \\displaystyle v_{2},w_{2}\\in S_{2} , entonces \\displaystyle v_{1}+v_{2}+w_{1}+w_{2}=\\underbrace{(v_{1}+w_{1})}_{\\in S_{1}}+% \\underbrace{(v_{2}+w_{2})}_{\\in S_{2}}\\in S_{1}+S_{2} 3.​Si \\displaystyle v_{1}+v_{2}\\in S_{1}+S_{2} , donde \\displaystyle v_{1}\\in S_{1} y \\displaystyle v_{2}\\in S_{2} , y \\displaystyle\\alpha\\in\\mathbb{K} entonces \\displaystyle\\alpha(v_{1}+v_{2})=\\underbrace{\\alpha v_{1}}_{\\in S_{1}}+% \\underbrace{\\alpha v_{2}}_{\\in S_{2}}\\in S_{1}+S_{2} ∎ Más en general, se puede considerar la suma de \\displaystyle n subespacios vectoriales, con \\displaystyle n\\in\\mathbb{N} . Definición 6.3 (Suma directa). Decimos que dos subespacios \\displaystyle S y \\displaystyle T suman de forma directa o que la suma de \\displaystyle S y \\displaystyle T es suma directa si \\displaystyle S\\cap T=\\{0\\} y se escribe \\displaystyle S\\oplus T . Si ademas \\displaystyle S\\oplus T=V decimos que \\displaystyle S y \\displaystyle T son suplementarios. Previous page Next page"],[["index.html","Pt2.html","S7.html"],"7 Combinaciones lineales, vectores linealmente independientes y bases ‣ Parte II Espacios vectoriales ‣ Álgebra lineal‣ Parte II Espacios vectoriales ‣ Álgebra lineal","Skip to content. Combinaciones lineales, vectores linealmente independientes y bases 7 Combinaciones lineales, vectores linealmente independientes y bases Definición 7.1 (Combinación lineal). Sean \\displaystyle v_{1},\\ldots,v_{k} vectores de \\displaystyle V . Decimos que el vector \\displaystyle v\\in V es combinación lineal de \\displaystyle v_{1},\\ldots,v_{k} si existen escalares \\displaystyle\\alpha_{1},\\ldots,\\alpha_{k}\\in\\mathbb{K} tales que \\displaystyle v=\\alpha_{1}v_{1}+\\cdots+\\alpha_{k}v_{k} Ejemplo. El vector \\displaystyle(1,2,3) es combinación lineal de los vectores \\displaystyle(1,1,1) , \\displaystyle(1,0,1) y \\displaystyle(0,1,1) : \\displaystyle(1,2,3)=x(1,1,1)+y(1,0,1)+z(0,1,1), \\displaystyle\\begin{dcases}x+y=1\\\\ x+z=2\\\\ x+y+z=3\\end{dcases}\\Rightarrow\\text{S.C.D} Luego \\displaystyle\\exists x,y,z\\in\\mathbb{K} tales que \\displaystyle v=xv_{1}+yv_{2}+zv_{3} . Lema 7.1. Dados los vectores \\displaystyle v_{1},\\ldots,v_{k}\\in V , el conjunto de todas las combinaciones lineales de \\displaystyle v_{1},\\ldots,v_{k} es un subespacio vectorial y lo denotamos por \\displaystyle\\langle v_{1},\\ldots,v_{k}\\rangle : \\displaystyle\\langle v_{1},\\ldots,v_{k}\\rangle=\\{\\alpha_{1}v_{1}+\\cdots\\alpha_% {k}v_{k}\\mid\\alpha_{1},\\ldots,\\alpha_{k}\\in\\mathbb{K}\\} Demostración. Por la caracterización de subespacios vectoriales. ∎ Definición 7.2. El subespacio \\displaystyle S=\\langle v_{1},\\ldots,v_{k}\\rangle se llama subespacio generado por los vectores \\displaystyle v_{1},\\ldots,v_{k} y a los vectores \\displaystyle v_{1},\\ldots,v_{k} se les llama sistema generador del subespacio \\displaystyle S . Dos familias de vectores son equivalentes si generan el mismo subespacio vectorial. Definición 7.3 (Linealmente independientes). Decimos que los vectores \\displaystyle v_{1},\\ldots,v_{k} son linealmente independientes cuando \\displaystyle\\alpha_{1}v_{1}+\\cdots+\\alpha_{k}v_{k}=0_{V}\\Rightarrow\\alpha_{1}% =0,\\ldots,\\alpha_{k}=0. Equivalentemente, son linealmente independientes cuando ninguno de ellos se puede expresar como combinación lineal de los demás. Cuando los vectores no son linealmente independientes decimos que son linealmente dependientes. Observación. 1.​Si \\displaystyle 0_{V} es uno de los vectores \\displaystyle v_{1},\\ldots,v_{k} , entonces siempre son linealmente dependientes. 2.​Si \\displaystyle v_{1},\\ldots,v_{k} son linealmente dependientes, entonces cualquier subconjunto de vectores escogidos entre \\displaystyle v_{1},\\ldots,v_{k} sigue siendo linealmente independiente. 3.​Si \\displaystyle v_{1},\\ldots,v_{k} son vectores linealmente dependientes, entonces si añadimos más vectores a \\displaystyle v_{1},\\ldots,v_{k} , éstos seguirán siendo linealmente dependientes. 4.​Si \\displaystyle v_{1},\\ldots,v_{k} son linealmente dependientes, entonces alguno de ellos se puede despejar como combinación lineal de los demás. 5.​Dos vectores \\displaystyle v_{1},v_{2} son linealmente independientes si uno no es múltiplo por un escalar del otro. Definición 7.4 (Base). Decimos que una familia de vectores \\displaystyle v_{1},\\ldots,v_{n} es una base de \\displaystyle V , y la denotamos por \\displaystyle\\mathcal{{B}}=\\{v_{1},\\ldots,v_{n}\\} si son linealmente independientes y son sistema generador (de \\displaystyle V ). En ese caso, todo vector \\displaystyle v de \\displaystyle V se expresa de forma única como combinación lineal de los vectores de \\displaystyle\\mathcal{{B}} : por ser sistema generador, existen \\displaystyle x_{1},\\ldots,x_{n}\\in\\mathbb{K} tales que \\displaystyle v=x_{1}v_{1}+\\cdots+x_{n}v_{n} . Además, estos escalares son únicos, puesto que si \\displaystyle x_{1}v_{1}+\\cdots x_{n}v_{n}=y_{1}v_{1}+\\cdots+x_{n}v_{n} entonces, restando, \\displaystyle(x_{1}-y_{1})v_{1}+\\cdots+(x_{n}-y_{n})v_{n}=0_{V} y, como son linealmente independientes, \\displaystyle x_{1}=y , \\displaystyle\\ldots , \\displaystyle x_{n}=y_{n} . A los únicos \\displaystyle(x_{1},\\ldots,x_{n})\\in\\mathbb{K}^{n} que permiten expresar \\displaystyle v como combinación lineal de los vectores de \\displaystyle\\mathcal{{B}} se les llaman coordenadas de \\displaystyle v respecto de la base \\displaystyle\\mathcal{{B}} . \\displaystyle(v)_{\\mathcal{{B}}}=(x_{1},\\ldots,x_{n}). Ejemplo. 1.​ \\displaystyle\\mathbb{K} como espacio vectorial sobre \\displaystyle\\mathbb{K} tiene de base a \\displaystyle\\mathcal{{B}}=\\{1\\} . Por ejemplo, \\displaystyle\\mathbb{C} como espacio vectorial sobre \\displaystyle\\mathbb{C} tiene de base a \\displaystyle\\mathcal{{B}}=\\{1\\} . Sin embargo, si consideramos \\displaystyle\\mathbb{C} como espacio vectorial sobre \\displaystyle\\mathbb{R} una base sería \\displaystyle\\mathcal{{B}}=\\{1,i\\} (necesitamos como mínimo dos vectores para tener un sistema generador). 2.​El espacio vectorial \\displaystyle V=\\{0_{V}\\} no tiene base porque no tiene ninguna familia de vectores linealmente independientes. Por convenio decimos que \\displaystyle\\mathcal{{B}}=\\varnothing es la base de este espacio vectorial. 3.​ \\displaystyle\\{1+x,1-x\\} es una base del espacio vectorial \\displaystyle\\mathbb{K}_{1}[x] :  ​Son sistema generador ya que cualquier polinomio de grado menor o igual que \\displaystyle 1 , \\displaystyle a+bx , se puede escribir como \\displaystyle a+bx=\\lambda(1+x)+\\mu(1-x) ya que el sistema (con incógnitas \\displaystyle\\lambda y \\displaystyle\\mu ) \\displaystyle\\begin{dcases}\\lambda+\\mu=a\\\\ \\lambda-\\mu-b\\end{dcases} es sistema compatible (de hecho, \\displaystyle\\lambda=\\frac{a+b}{2},\\mu=\\frac{a-b}{2} es solución del sistema).  ​Son linealmente independientes ya que si \\displaystyle\\lambda(1+x)+\\mu(1-x)=0 entonces \\displaystyle\\begin{dcases}\\lambda+\\mu=0\\\\ \\lambda-\\mu=0\\end{dcases} y la única solución del sistema es \\displaystyle\\lambda=0,\\mu=0 . Ejemplo. Se llaman bases canónicas a las bases más utilizadas de los espacios vectoriales \\displaystyle\\mathbb{K}^{n},\\mathbb{K}[x],\\mathbb{K}_{n}[x] y \\displaystyle\\mathcal{{M}}_{m\\times n}(\\mathbb{K}) :  ​Base canónica de \\displaystyle\\mathbb{K}^{n}:\\mathcal{{BC}}=\\{(1,0,\\ldots,0),(0,1,\\ldots,0),% \\ldots,(0,\\ldots,0,1)\\} .  ​Base canónica de \\displaystyle\\mathbb{K}[x]:\\mathcal{{BC}}=\\{1,x,x^{2},\\ldots,x^{k},\\ldots\\} .  ​Base canónica de \\displaystyle\\mathbb{K}_{n}[x]:\\mathcal{{BC}}=\\{1,x,x^{2},\\ldots,x^{n}\\} .  ​Base canónica de \\displaystyle\\mathcal{{M}}_{m\\times n}(\\mathbb{K}):\\mathcal{{BC}}=\\{E_{ij}\\mid i% =1,\\ldots,m,\\;j=1,\\ldots,n\\} . Definición 7.5. Decimos que \\displaystyle V es finitamente generado si posee algún sistema generador finito. Lema 7.2 (Existencia de bases en espacios vectoriales finitamente generados). Toda familia de vectores \\displaystyle v_{1},\\ldots,v_{m} posee una subfamilia formada por vectores linealmente independientes que es equivalente a la original. En particular, todo espacio vectorial finitamente generado posee una base. Demostración. Si los vectores \\displaystyle v_{1},\\ldots,v_{m} son linealmente independientes, ya está el resultado. Si no, existe al menos una combinación lineal de esos vectores, con escalares no todos nulos, que hace que \\displaystyle\\alpha_{1}v_{1}+\\cdots+\\underbrace{\\alpha_{i}v_{i}}_{\\neq 0}+% \\cdots+\\alpha_{m}v_{m}=0_{V}. Supongamos que \\displaystyle\\alpha_{i}\\neq 0 , así que podemos despejar y obtener \\displaystyle v_{i}=-\\frac{1}{\\alpha_{i}}(\\alpha_{1}v_{1}+\\cdots+\\cancel{% \\alpha_{i}v_{i}}+\\cdots+\\alpha_{m}v_{m}). Como el vector \\displaystyle v_{i} es combinación lineal de los demás, \\displaystyle\\langle v_{1},\\ldots,v_{i},\\ldots,v_{m}\\rangle=\\langle v_{1},% \\ldots,\\cancel{v_{i}},\\ldots,v_{m}\\rangle, con lo que obtenemos una familia que genera el mismo subespacio pero con un elemento menos. Repetimos el proceso hasta obtener una subfamilia equivalente cuyos vectores son linealmente independientes. ∎ Lema 7.3. Si los vectores \\displaystyle v_{1},\\ldots,v_{m} son linealmente dependientes y \\displaystyle v_{1}\\neq 0_{V} entonces existe algún \\displaystyle v_{i} que se puede poner como combinación lineal de \\displaystyle v_{1},\\ldots,v_{i-1} , es decir, uno de los vectores se puede expresar como combinación lineal de los anteriores. Demostración. Sabemos que existen escalares no todos nulos tal que \\displaystyle\\alpha_{1}v_{1}+\\cdots+\\alpha_{m}v_{m}=0_{V} . Como \\displaystyle v_{1}\\neq 0_{V} , existe \\displaystyle i>1 tal que \\displaystyle\\alpha_{1}v_{1}+\\cdots+\\underbrace{\\alpha_{i}}_{\\neq 0}v_{i}=0_{V} Despejamos \\displaystyle v_{i} como \\displaystyle v_{i}=-\\frac{1}{\\alpha_{i}}(\\alpha_{1}v_{1}+\\cdots+\\alpha_{i-1}v% _{i-1}) ∎ Proposición 7.1. Sea \\displaystyle v_{1},\\ldots,v_{n} una familia de vectores linealmente independientes y \\displaystyle w_{1},\\ldots w_{m} una familia de vectores que son sistema generador de \\displaystyle V . Entonces \\displaystyle n\\leq m . Demostración. Tomamos el primer vector de la familia de vectores linealmente independientes, \\displaystyle v_{1} . Como \\displaystyle v_{1}\\in V=\\langle w_{1},\\ldots,w_{m}\\rangle se tiene que los vectores \\displaystyle v_{1},w_{1},\\ldots,w_{m} son necesariamente linealmente independientes (uno de ellos, el primero, es combinación lineal de los demás). Por el lema anterior, como \\displaystyle v_{1}\\neq 0_{V} existe un \\displaystyle w_{i} que se escribe como combinación lineal de los anteriores. Lo eliminamos y repetimos el proceso, tomando ahora el segundo vector de la familia de vectores linealmente independientes, \\displaystyle v_{2} , y el sistema generador de \\displaystyle V formado por: \\displaystyle v_{1},w_{1},\\ldots,\\cancel{w_{i}},\\ldots,w_{m}. De esta forma, vamos insertando “por delante” vectores del tipo \\displaystyle v_{i} en el sistema generador y eliminando vectores de tipo \\displaystyle w_{j} . Si hubiera más \\displaystyle v ’s que \\displaystyle w ’s (si \\displaystyle n:m ) llegaríamos a un sistema generador de la forma \\displaystyle v_{1},\\ldots,v_{m} de forma que \\displaystyle v_{m+1}\\in\\langle v_{1},\\ldots,v_{m}\\rangle , lo cual no es posible porque los vectores \\displaystyle v_{1},\\ldots,v_{n} son linealmente independientes por hipótesis. Llegamos, por tanto, a que \\displaystyle n\\leq m . ∎ Teorema 7.4. Todas las bases de un mismo espacio vectorial \\displaystyle V finitamente generado tienen el mismo número de elementos. A este número se le llama dimensión de \\displaystyle V y se denota \\displaystyle\\dim_{K}V . Demostración. Sean \\displaystyle\\mathcal{{B}}=\\{v_{1},\\ldots,v_{n}\\} y \\displaystyle\\mathcal{{C}}=\\{w_{1},\\ldots,w_{n}\\} dos bases de \\displaystyle V . Veamos que \\displaystyle n=m .  ​Como \\displaystyle v_{1},\\ldots,v_{n} son linealmente independientes y \\displaystyle w_{1},\\ldots,w_{m} son un sistema generador, \\displaystyle n\\leq m .  ​Como \\displaystyle w_{1},\\ldots,w_{m} son linealmente independientes y \\displaystyle v_{1},\\ldots,v_{n} son un sistema generador, \\displaystyle m\\leq n . Por tanto, \\displaystyle n=m . ∎ Como conocemos bases canónicas para los espacios vectoriales de tipo \\displaystyle\\mathbb{K}^{n},\\mathbb{K}_{n}[x] y \\displaystyle\\mathcal{{M}}_{m\\times n}(\\mathbb{K}) , conocemos sus dimensiones:  ​ \\displaystyle\\dim\\mathbb{K}^{n}=n ,  ​ \\displaystyle\\dim\\mathbb{K}_{n}[x]=n+1 ,  ​ \\displaystyle\\dim\\mathcal{{M}}_{m\\times n}(\\mathbb{K})=mn Proposición 7.2. Sea \\displaystyle V un espacio vectorial de dimensión \\displaystyle n . Son equivalentes: 1.​ \\displaystyle v_{1},\\ldots,v_{n} es una base de \\displaystyle V 2.​ \\displaystyle v_{1},\\ldots,v_{n} es un sistema generador de \\displaystyle V 3.​ \\displaystyle v_{1},\\ldots,v_{n} son linealmente independientes Demostración. \\displaystyle 1)\\Rightarrow 2) y \\displaystyle 1)\\Rightarrow 3) Obvio. \\displaystyle 2)\\Rightarrow 1) Por reducción al absurdo, si \\displaystyle v_{1},\\ldots,v_{n} no fueran linealmente independientes, por el Lema 7.2 se puede extraer una subfamilia con vectores linealmente independientes y que genere el mismo espacio que \\displaystyle v_{1},\\ldots,v_{n} , con lo que obtendríamos una base de \\displaystyle V con un número de vectores más pequeño que \\displaystyle n . Esto es una contradicción. \\displaystyle 3)\\Rightarrow 1) Por reducción al absurdo, si \\displaystyle v_{1},\\ldots,v_{n} no fueran sistema generador, habría algún vector \\displaystyle v\\in V que no se pueda poner como combinación lineal de \\displaystyle v_{1},\\ldots,v_{n} , así que los vectores \\displaystyle v_{1},\\ldots,v_{n},v seguirían siendo linealmente independientes y obtendríamos \\displaystyle n+1 vectores linealmente independientes. Por otra parte, como \\displaystyle\\dim V=n , tiene una base \\displaystyle\\mathcal{{V}}=\\{w_{1},\\ldots,w_{n}\\} . Los vectores de \\displaystyle\\mathcal{{B}} son un sistema generador y estamos obteniendo \\displaystyle n+1 vectores linealmente independientes y un sistema generador con \\displaystyle n vectores. Esto es una contradicción con la Proposición 7.1. ∎ Lema 7.5. Sea \\displaystyle V un espacio vectorial finitamente generado. Toda familia de vectores linealmente independientes se puede completar hasta una base de \\displaystyle V . Demostración. Sean \\displaystyle w_{1},\\ldots,w_{k} vectores linealmente independientes. Se toma una base \\displaystyle\\{v_{1},\\ldots,v_{n}\\} de \\displaystyle V como base de referencia y van añadiendo a \\displaystyle w_{1},\\ldots,w_{k} vectores de esta base de referencia, de uno en uno, comprobando en cada paso que seguimos teniendo vectores linealmente independientes. El proceso termina cuando tengamos \\displaystyle n vectores linealmente independientes, que son necesariamente una base por la Proposición 7.2. ∎ Corolario 7.1. Sea \\displaystyle S un subespacio vectorial de \\displaystyle V . Entonces \\displaystyle\\dim S\\leq\\dim V . Además, \\displaystyle\\dim S=\\dim V\\Leftrightarrow S=V Demostración. Sea \\displaystyle\\mathcal{{B}}_{s} una base de \\displaystyle S . Como estos vectores son linealmente independientes, se pueden completar a una base de \\displaystyle V por el Lema 7.5, así que \\displaystyle s\\leq\\dim V . Además, si \\displaystyle\\dim S=\\dim V , cualquier base de \\displaystyle S tiene \\displaystyle n=\\dim V vectores linealmente independientes, y por la Proposición 7.2 estos vectores son una base de \\displaystyle V . El subespacio generado por ellos da, por una parte \\displaystyle S , por ser base de \\displaystyle S , y por otra parte \\displaystyle V , por ser base de \\displaystyle V , con lo que \\displaystyle S=V . La otra implicación es obvia. ∎ Teorema 7.6. Sean \\displaystyle S y \\displaystyle T dos subespacios de \\displaystyle V . Entonces \\displaystyle\\dim(S+T)=\\dim S+\\dim T-\\dim(S\\cap T) Demostración. Si \\displaystyle\\dim S=0 entonces \\displaystyle S=\\{0_{V}\\} , en cuyo caso \\displaystyle S+T=T,S\\cap T=\\{0_{V}\\} y la fórmula se cumple porque \\displaystyle\\dim(S+T)=\\dim T=\\underbrace{\\dim S}_{0}+\\dim T-\\underbrace{\\dim(% S\\cap T)}_{0} Lo mismo ocurriría si \\displaystyle\\dim T=0 . Supongamos que \\displaystyle\\dim S=s>0 y \\displaystyle\\dim T=t>0 . Empezamos con una base \\displaystyle\\mathcal{{B}}_{S\\cap T}=\\{v_{1},\\ldots,v_{r}\\} de \\displaystyle S\\cap T .  ​Por un lado, completamos la base \\displaystyle\\mathcal{{B}}_{S\\cap T} a una base de \\displaystyle S : \\displaystyle\\{v_{1},\\ldots,v_{r},v_{r+1},\\ldots v_{s}\\} .  ​Por otro lado, completamos la base \\displaystyle\\mathcal{{B}}_{S\\cap T} a una base de \\displaystyle T : \\displaystyle\\{v_{1},\\ldots,v_{r},w_{r+1},\\ldots,w_{t}\\} . Veamos que \\displaystyle\\{v_{1},\\ldots,v_{r},v_{r+1},\\ldots,v_{s},w_{r+1},\\ldots,w_{t}\\} es una base de \\displaystyle S+T (si probamos esto, estaremos viendo que \\displaystyle S+T tiene una base con \\displaystyle s+t-r vectores, que es la fórmula que queremos demostrar):  ​Son linealmente independientes: escribimos \\displaystyle\\alpha_{1}v_{1}+\\cdots+\\alpha_{s}v_{s}+\\beta_{r+1}w_{r+1}+\\cdots+% \\beta_{t}w_{t}=0_{V}. Despejando, tenemos \\displaystyle\\underbrace{\\alpha_{i}v_{i}+\\cdots+\\alpha_{s}v_{s}}_{\\in S}=-% \\underbrace{(\\beta_{r+1}w_{r+1}+\\cdots+\\beta_{t}w_{t})}_{\\in T}\\in S\\cap T, y como \\displaystyle\\mathcal{{B}}_{S\\cap T}=\\{v_{1},\\ldots,v_{r}\\} es una base de \\displaystyle S\\cap T , existen escalares \\displaystyle\\lambda_{1},\\ldots,\\lambda_{r} tales que \\displaystyle-(\\beta_{r+1}w_{r+1}+\\cdots+\\beta_{t}w_{t})=\\lambda_{1}v_{1}+% \\cdots+\\lambda_{r}v_{r} . Luego \\displaystyle\\lambda_{1}v_{1}+\\cdots+\\lambda_{r}v_{r}+\\beta_{r+1}w_{r+1}+% \\cdots+\\beta_{t}w_{t}=0_{V} y usando que los vectores \\displaystyle v_{1},\\ldots,v_{r},w_{r+1},w_{t} son linealmente independientes (base de \\displaystyle T ) llegamos a \\displaystyle\\lambda_{1}=0,\\ldots,\\lambda_{r}=0,\\boxed{\\beta_{r+1}=0},\\ldots,% \\boxed{\\beta_{t}=0}. Sustituimos en la ecuación inicial y llegamos a \\displaystyle\\alpha_{1}v_{1}+\\cdots+\\alpha_{s}v_{s}=0_{V}. y como \\displaystyle v_{1},\\ldots,v_{s} son linealmente independientes (base de \\displaystyle S ), resulta que \\displaystyle\\boxed{\\alpha_{1}=0},\\ldots,\\boxed{\\alpha_{s}=0}.  ​Son sistema generador: \\displaystyle\\displaystyle S \\displaystyle\\displaystyle=\\langle v_{1},\\ldots,v_{r},v_{r+1},\\ldots,v_{s}\\rangle \\displaystyle\\displaystyle T \\displaystyle\\displaystyle=\\langle v_{1},\\ldots,v_{r},\\ldots,w_{r+1},\\ldots,w_% {t}\\rangle con lo que \\displaystyle S+T=\\langle v_{1},\\ldots,v_{r},v_{r+1},\\ldots,v_{s},w_{r+1},% \\ldots,w_{r}\\rangle ∎ Previous page Next page"],[["index.html","Pt2.html","S8.html"],"8 Espacio cociente ‣ Parte II Espacios vectoriales ‣ Álgebra lineal‣ Parte II Espacios vectoriales ‣ Álgebra lineal","Skip to content. Espacio cociente 8 Espacio cociente Definición 8.1. Sea \\displaystyle S un subespacio vectorial de \\displaystyle V . En \\displaystyle V definimos la siguiente relación \\displaystyle v\\sim w\\text{ si }v-w\\in S. Proposición 8.1. La relación \\displaystyle\\sim es una relación de equivalencia. Demostración.  ​Reflexiva: para cada \\displaystyle v,v\\sim v porque \\displaystyle v-v=0_{V}\\in S  ​Simétrica: \\displaystyle v\\sim w\\Rightarrow v-w\\in S\\Rightarrow w-v\\in S\\Rightarrow w\\sim v  ​Transitiva: \\displaystyle\\begin{rcases}v\\sim w\\Rightarrow v-w\\in S\\\\ w\\sim u\\Rightarrow w-u\\in S\\end{rcases}\\Rightarrow(v-w)+(w-u)=v-u\\in S% \\Rightarrow v\\sim u ∎ Proposición 8.2. Las clases de equivalencia respecto de la relación \\displaystyle\\sim son \\displaystyle[v]_{\\sim}=\\{v+s\\mid s\\in S\\}=v+S Demostración. Por doble contenido: \\displaystyle\\boxed{\\subseteq} Si \\displaystyle w\\in[v] , \\displaystyle w\\sim v así que \\displaystyle w-v=s\\in S , y \\displaystyle w=v+s\\in v+S . \\displaystyle\\boxed{\\supseteq} Si tomamos \\displaystyle w=v+s para algún \\displaystyle s\\in S entonces \\displaystyle w-v=s\\in S , luego \\displaystyle w\\sim v\\Rightarrow w\\in[v] . ∎ El conjunto cociente \\displaystyle V/\\sim lo vamos a denotar como \\displaystyle V/S y va a estar formado por las clases de equivalencia: \\displaystyle V/S=\\{v+S\\mid v\\in S\\} Proposición 8.3. En el conjunto cociente \\displaystyle V/S podemos definir las operaciones \\displaystyle+\\colon V/S\\times V/S \\displaystyle\\longrightarrow V/S \\displaystyle(v+S,w+S) \\displaystyle\\longmapsto(v+w)+S y \\displaystyle\\cdot_{K}\\colon\\mathbb{K}\\times V/S \\displaystyle\\longrightarrow V/S \\displaystyle(\\lambda,v+S) \\displaystyle\\longmapsto(\\lambda v)+S Estas operaciones están bien definidas y dan a \\displaystyle V/S una estructura de espacio vectorial. Este espacio vectorial se llama espacio cociente. Demostración. Veamos que \\displaystyle+ está bien definida, es decir, si \\displaystyle v+S=\\hat{v}+S y \\displaystyle w+S=\\hat{w}+S entonces \\displaystyle(v+w)+S=(\\hat{v}+\\hat{w})+S : \\displaystyle\\displaystyle v+S=\\hat{v}+S\\Rightarrow v-\\hat{v}\\in S \\displaystyle\\displaystyle w+S=\\hat{w}+S\\Rightarrow w-\\hat{w}\\in S así que \\displaystyle v+w-(\\hat{v}+\\hat{w})=\\underbrace{v-\\hat{v}}_{\\in S}+\\underbrace% {w-\\hat{w}}_{\\in S}\\in S. Veamos que \\displaystyle\\cdot_{K} está bien definida, es decir, si \\displaystyle v+S=\\hat{v}+S y \\displaystyle\\lambda\\in\\mathbb{K} entonces \\displaystyle(\\lambda v)+S=(\\lambda\\hat{v})+S : \\displaystyle v+S=\\hat{v}+S\\Rightarrow v-\\hat{v}\\in S así que \\displaystyle(\\lambda v)-(\\lambda\\hat{v})=\\lambda\\underbrace{v-\\hat{v}}_{\\in S% }\\in S Se puede comprobar que \\displaystyle(V/S,+) es un grupo abeliano. El elemento neutro de la \\displaystyle+ es la clase de equivalencia \\displaystyle 0_{V}+S=S y el opuesto de cada clase de equivalencia \\displaystyle v+S es \\displaystyle(-v)+S . El resto de las propiedades se sigue de forma inmediata. ∎ Previous page Next page"],[["index.html"],"Álgebra lineal","Skip to content. Álgebra lineal Álgebra lineal Diego Rodriguez Universidad Rey Juan Carlos Matemáticas + Ingeniería Informática Curso 2023-2024 Álgebra lineal © 2024 by Diego Rodríguez is licensed under Attribution-NonCommercial 4.0 International. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/ Next page"]]